
-------------   Fold 1 / 5  -------------

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-28/1/pretrained_best_fold0.h5

 -> Training Model 

Epoch 1/60
60/60 - 11s - loss: 0.2565 - lwlrap: 0.4633 - val_loss: 0.1756 - val_lwlrap: 0.5973
Epoch 2/60
60/60 - 10s - loss: 0.1286 - lwlrap: 0.7714 - val_loss: 0.1615 - val_lwlrap: 0.7029
Epoch 3/60
60/60 - 10s - loss: 0.1052 - lwlrap: 0.7675 - val_loss: 0.1529 - val_lwlrap: 0.6307
Epoch 4/60
60/60 - 10s - loss: 0.1809 - lwlrap: 0.7947 - val_loss: 0.1320 - val_lwlrap: 0.6743
Epoch 5/60
60/60 - 10s - loss: 0.0941 - lwlrap: 0.7928 - val_loss: 0.1703 - val_lwlrap: 0.6556
Epoch 6/60
60/60 - 10s - loss: 0.0746 - lwlrap: 0.8487 - val_loss: 0.0677 - val_lwlrap: 0.6937
Epoch 7/60
60/60 - 10s - loss: 0.0585 - lwlrap: 0.9073 - val_loss: 0.2559 - val_lwlrap: 0.8062
Epoch 8/60
60/60 - 10s - loss: 0.0197 - lwlrap: 0.9314 - val_loss: 0.1697 - val_lwlrap: 0.8203
Epoch 9/60
60/60 - 10s - loss: 0.0197 - lwlrap: 0.9658 - val_loss: 0.1997 - val_lwlrap: 0.8421
Epoch 10/60
60/60 - 10s - loss: 0.0178 - lwlrap: 0.9789 - val_loss: 0.1585 - val_lwlrap: 0.8384
Epoch 11/60
60/60 - 10s - loss: 0.0147 - lwlrap: 0.9855 - val_loss: 0.1123 - val_lwlrap: 0.8464
Epoch 12/60
60/60 - 10s - loss: 0.0089 - lwlrap: 0.9845 - val_loss: 0.1105 - val_lwlrap: 0.8374
Epoch 13/60
60/60 - 10s - loss: 0.0069 - lwlrap: 0.9880 - val_loss: 0.1797 - val_lwlrap: 0.8509
Epoch 14/60
60/60 - 10s - loss: 0.0083 - lwlrap: 0.9806 - val_loss: 0.2508 - val_lwlrap: 0.8314
Epoch 15/60
60/60 - 10s - loss: 0.0184 - lwlrap: 0.9672 - val_loss: 0.1556 - val_lwlrap: 0.7832
Epoch 16/60
60/60 - 10s - loss: 0.0025 - lwlrap: 0.9732 - val_loss: 0.2559 - val_lwlrap: 0.8064
Epoch 17/60
60/60 - 10s - loss: 0.0177 - lwlrap: 0.9821 - val_loss: 0.3634 - val_lwlrap: 0.8243
Epoch 18/60
60/60 - 10s - loss: 0.0100 - lwlrap: 0.9879 - val_loss: 0.3049 - val_lwlrap: 0.8234
Epoch 19/60
60/60 - 10s - loss: 0.0464 - lwlrap: 0.9901 - val_loss: 0.2109 - val_lwlrap: 0.8426
Epoch 20/60
60/60 - 10s - loss: 0.0128 - lwlrap: 0.9948 - val_loss: 0.1050 - val_lwlrap: 0.8682
Epoch 21/60
60/60 - 10s - loss: 0.0109 - lwlrap: 0.9981 - val_loss: 0.0684 - val_lwlrap: 0.8552
Epoch 22/60
60/60 - 10s - loss: 0.0130 - lwlrap: 0.9965 - val_loss: 0.2853 - val_lwlrap: 0.8431
Epoch 23/60
60/60 - 10s - loss: 0.0095 - lwlrap: 0.9961 - val_loss: 0.2217 - val_lwlrap: 0.8416
Epoch 24/60
60/60 - 10s - loss: 0.0071 - lwlrap: 0.9955 - val_loss: 0.2026 - val_lwlrap: 0.8317
Epoch 25/60
60/60 - 10s - loss: 8.3785e-04 - lwlrap: 0.9970 - val_loss: 0.3202 - val_lwlrap: 0.8312
Epoch 26/60
60/60 - 10s - loss: 0.0029 - lwlrap: 0.9909 - val_loss: 0.3995 - val_lwlrap: 0.8145
Epoch 27/60
60/60 - 10s - loss: 0.0137 - lwlrap: 0.9945 - val_loss: 0.2777 - val_lwlrap: 0.8245
Epoch 28/60
60/60 - 10s - loss: 0.0022 - lwlrap: 0.9975 - val_loss: 0.2496 - val_lwlrap: 0.8285
Epoch 29/60
60/60 - 10s - loss: 3.3482e-04 - lwlrap: 0.9977 - val_loss: 0.1665 - val_lwlrap: 0.8398
Epoch 30/60
60/60 - 10s - loss: 4.0479e-04 - lwlrap: 0.9980 - val_loss: 0.0462 - val_lwlrap: 0.8537
Epoch 31/60
60/60 - 10s - loss: 0.0017 - lwlrap: 0.9975 - val_loss: 0.1322 - val_lwlrap: 0.8460
Epoch 32/60
60/60 - 10s - loss: 5.4047e-04 - lwlrap: 1.0000 - val_loss: 0.2146 - val_lwlrap: 0.8532
Epoch 33/60
60/60 - 10s - loss: 0.0139 - lwlrap: 0.9985 - val_loss: 0.5126 - val_lwlrap: 0.8417
Epoch 34/60
60/60 - 10s - loss: 9.0606e-04 - lwlrap: 0.9982 - val_loss: 0.4563 - val_lwlrap: 0.8285
Epoch 35/60
60/60 - 10s - loss: 0.0011 - lwlrap: 0.9962 - val_loss: 0.3832 - val_lwlrap: 0.8342
Epoch 36/60
60/60 - 10s - loss: 0.0011 - lwlrap: 0.9975 - val_loss: 0.3469 - val_lwlrap: 0.8406
Epoch 37/60
60/60 - 10s - loss: 0.0062 - lwlrap: 0.9977 - val_loss: 0.2297 - val_lwlrap: 0.8401
Epoch 38/60
60/60 - 10s - loss: 3.2334e-04 - lwlrap: 0.9988 - val_loss: 0.3832 - val_lwlrap: 0.8561
Epoch 39/60
60/60 - 10s - loss: 2.8775e-04 - lwlrap: 0.9984 - val_loss: 0.3917 - val_lwlrap: 0.8452
Epoch 40/60
60/60 - 10s - loss: 7.4563e-04 - lwlrap: 0.9991 - val_loss: 0.2342 - val_lwlrap: 0.8415

-------------   Fold 2 / 5  -------------

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-28/1/pretrained_best_fold1.h5

 -> Training Model 

Epoch 1/60
60/60 - 11s - loss: 0.1848 - lwlrap: 0.5728 - val_loss: 0.1201 - val_lwlrap: 0.7865
Epoch 2/60
60/60 - 9s - loss: 0.0573 - lwlrap: 0.8822 - val_loss: 0.0790 - val_lwlrap: 0.7473
Epoch 3/60
60/60 - 9s - loss: 0.0608 - lwlrap: 0.8548 - val_loss: 0.1080 - val_lwlrap: 0.7017
Epoch 4/60
60/60 - 9s - loss: 0.0891 - lwlrap: 0.8631 - val_loss: 0.0750 - val_lwlrap: 0.7662
Epoch 5/60
60/60 - 10s - loss: 0.0457 - lwlrap: 0.8609 - val_loss: 0.0586 - val_lwlrap: 0.7905
Epoch 6/60
60/60 - 10s - loss: 0.1076 - lwlrap: 0.8617 - val_loss: 0.0951 - val_lwlrap: 0.7931
Epoch 7/60
60/60 - 10s - loss: 0.0511 - lwlrap: 0.9159 - val_loss: 0.0530 - val_lwlrap: 0.8426
Epoch 8/60
60/60 - 10s - loss: 0.0222 - lwlrap: 0.9558 - val_loss: 0.0545 - val_lwlrap: 0.8704
Epoch 9/60
60/60 - 10s - loss: 0.0405 - lwlrap: 0.9764 - val_loss: 0.0619 - val_lwlrap: 0.8869
Epoch 10/60
60/60 - 10s - loss: 0.0207 - lwlrap: 0.9828 - val_loss: 0.0653 - val_lwlrap: 0.9035
Epoch 11/60
60/60 - 9s - loss: 0.0271 - lwlrap: 0.9844 - val_loss: 0.0659 - val_lwlrap: 0.8982
Epoch 12/60
60/60 - 9s - loss: 0.0147 - lwlrap: 0.9906 - val_loss: 0.0568 - val_lwlrap: 0.8847
Epoch 13/60
60/60 - 9s - loss: 0.0432 - lwlrap: 0.9878 - val_loss: 0.0555 - val_lwlrap: 0.8882
Epoch 14/60
60/60 - 9s - loss: 0.0387 - lwlrap: 0.9924 - val_loss: 0.0759 - val_lwlrap: 0.8768
Epoch 15/60
60/60 - 10s - loss: 0.0638 - lwlrap: 0.9741 - val_loss: 0.0742 - val_lwlrap: 0.7848
Epoch 16/60
60/60 - 9s - loss: 0.0169 - lwlrap: 0.9747 - val_loss: 0.0356 - val_lwlrap: 0.8189
Epoch 17/60
60/60 - 9s - loss: 0.0100 - lwlrap: 0.9817 - val_loss: 0.0352 - val_lwlrap: 0.8765
Epoch 18/60
60/60 - 9s - loss: 0.0086 - lwlrap: 0.9854 - val_loss: 0.0234 - val_lwlrap: 0.8951
Epoch 19/60
60/60 - 9s - loss: 0.0032 - lwlrap: 0.9941 - val_loss: 0.0476 - val_lwlrap: 0.8852
Epoch 20/60
60/60 - 9s - loss: 0.0034 - lwlrap: 0.9940 - val_loss: 0.0333 - val_lwlrap: 0.8994
Epoch 21/60
60/60 - 9s - loss: 0.0119 - lwlrap: 0.9973 - val_loss: 0.0442 - val_lwlrap: 0.9020
Epoch 22/60
60/60 - 9s - loss: 6.9254e-04 - lwlrap: 0.9983 - val_loss: 0.0208 - val_lwlrap: 0.9013
Epoch 23/60
60/60 - 9s - loss: 0.0053 - lwlrap: 0.9974 - val_loss: 0.0205 - val_lwlrap: 0.8925
Epoch 24/60
60/60 - 9s - loss: 0.0145 - lwlrap: 0.9978 - val_loss: 0.0486 - val_lwlrap: 0.8712
Epoch 25/60
60/60 - 9s - loss: 0.0422 - lwlrap: 0.9979 - val_loss: 0.0280 - val_lwlrap: 0.8847
Epoch 26/60
60/60 - 9s - loss: 0.0027 - lwlrap: 0.9959 - val_loss: 0.0021 - val_lwlrap: 0.8639
Epoch 27/60
60/60 - 9s - loss: 0.0022 - lwlrap: 0.9988 - val_loss: 0.0111 - val_lwlrap: 0.8781
Epoch 28/60
60/60 - 9s - loss: 6.0354e-04 - lwlrap: 0.9983 - val_loss: 0.0103 - val_lwlrap: 0.8739
Epoch 29/60
60/60 - 9s - loss: 0.0014 - lwlrap: 0.9974 - val_loss: 0.0063 - val_lwlrap: 0.8735
Epoch 30/60
60/60 - 9s - loss: 0.0036 - lwlrap: 0.9988 - val_loss: 0.0099 - val_lwlrap: 0.8847

-------------   Fold 3 / 5  -------------

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-28/1/pretrained_best_fold2.h5

 -> Training Model 

Epoch 1/60
60/60 - 11s - loss: 0.1236 - lwlrap: 0.6331 - val_loss: 0.0852 - val_lwlrap: 0.7572
Epoch 2/60
60/60 - 10s - loss: 0.0894 - lwlrap: 0.8954 - val_loss: 0.0939 - val_lwlrap: 0.7153
Epoch 3/60
60/60 - 10s - loss: 0.1907 - lwlrap: 0.8279 - val_loss: 0.0819 - val_lwlrap: 0.7234
Epoch 4/60
60/60 - 10s - loss: 0.1012 - lwlrap: 0.8700 - val_loss: 0.0904 - val_lwlrap: 0.6307
Epoch 5/60
60/60 - 10s - loss: 0.1637 - lwlrap: 0.8609 - val_loss: 0.0664 - val_lwlrap: 0.7416
Epoch 6/60
60/60 - 10s - loss: 0.0790 - lwlrap: 0.8680 - val_loss: 0.0446 - val_lwlrap: 0.8015
Epoch 7/60
60/60 - 10s - loss: 0.0717 - lwlrap: 0.9457 - val_loss: 0.0614 - val_lwlrap: 0.7983
Epoch 8/60
60/60 - 10s - loss: 0.0102 - lwlrap: 0.9557 - val_loss: 0.0174 - val_lwlrap: 0.7986
Epoch 9/60
60/60 - 10s - loss: 0.0299 - lwlrap: 0.9761 - val_loss: 0.0264 - val_lwlrap: 0.8687
Epoch 10/60
60/60 - 10s - loss: 0.0042 - lwlrap: 0.9868 - val_loss: 0.0355 - val_lwlrap: 0.8315
Epoch 11/60
60/60 - 10s - loss: 0.0260 - lwlrap: 0.9927 - val_loss: 0.0248 - val_lwlrap: 0.8565
Epoch 12/60
60/60 - 10s - loss: 0.0266 - lwlrap: 0.9954 - val_loss: 0.0303 - val_lwlrap: 0.8568
Epoch 13/60
60/60 - 10s - loss: 0.0186 - lwlrap: 0.9886 - val_loss: 0.0576 - val_lwlrap: 0.8507
Epoch 14/60
60/60 - 10s - loss: 0.0186 - lwlrap: 0.9893 - val_loss: 0.0621 - val_lwlrap: 0.8349
Epoch 15/60
60/60 - 10s - loss: 0.0311 - lwlrap: 0.9820 - val_loss: 0.0274 - val_lwlrap: 0.8238
Epoch 16/60
60/60 - 10s - loss: 0.0358 - lwlrap: 0.9618 - val_loss: 0.0882 - val_lwlrap: 0.8111
Epoch 17/60
60/60 - 10s - loss: 0.0066 - lwlrap: 0.9779 - val_loss: 0.0258 - val_lwlrap: 0.8621
Epoch 18/60
60/60 - 10s - loss: 0.0044 - lwlrap: 0.9915 - val_loss: 0.0161 - val_lwlrap: 0.8629
Epoch 19/60
60/60 - 10s - loss: 0.0123 - lwlrap: 0.9919 - val_loss: 0.0176 - val_lwlrap: 0.8343
Epoch 20/60
60/60 - 10s - loss: 0.0180 - lwlrap: 0.9971 - val_loss: 0.0054 - val_lwlrap: 0.8662
Epoch 21/60
60/60 - 10s - loss: 0.0079 - lwlrap: 0.9981 - val_loss: 0.0043 - val_lwlrap: 0.8496
Epoch 22/60
60/60 - 10s - loss: 0.0070 - lwlrap: 0.9983 - val_loss: 0.0094 - val_lwlrap: 0.8439
Epoch 23/60
60/60 - 10s - loss: 0.0146 - lwlrap: 0.9971 - val_loss: 0.0436 - val_lwlrap: 0.8432
Epoch 24/60
60/60 - 10s - loss: 0.0021 - lwlrap: 0.9985 - val_loss: 0.0432 - val_lwlrap: 0.8570
Epoch 25/60
60/60 - 10s - loss: 0.0051 - lwlrap: 0.9979 - val_loss: 0.0760 - val_lwlrap: 0.8491
Epoch 26/60
60/60 - 10s - loss: 4.1498e-04 - lwlrap: 0.9970 - val_loss: 0.0387 - val_lwlrap: 0.8299
Epoch 27/60
60/60 - 10s - loss: 9.0503e-04 - lwlrap: 0.9978 - val_loss: 0.0138 - val_lwlrap: 0.8482
Epoch 28/60
60/60 - 10s - loss: 0.0015 - lwlrap: 0.9984 - val_loss: 0.0153 - val_lwlrap: 0.8399
Epoch 29/60
60/60 - 10s - loss: 0.0012 - lwlrap: 0.9978 - val_loss: 0.0416 - val_lwlrap: 0.8412

-------------   Fold 4 / 5  -------------

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-28/1/pretrained_best_fold3.h5

 -> Training Model 

Epoch 1/60
60/60 - 11s - loss: 0.1146 - lwlrap: 0.5981 - val_loss: 0.1656 - val_lwlrap: 0.7291
Epoch 2/60
60/60 - 9s - loss: 0.0774 - lwlrap: 0.8920 - val_loss: 0.1298 - val_lwlrap: 0.6962
Epoch 3/60
60/60 - 9s - loss: 0.1386 - lwlrap: 0.7744 - val_loss: 0.1757 - val_lwlrap: 0.7577
Epoch 4/60
60/60 - 9s - loss: 0.0691 - lwlrap: 0.8582 - val_loss: 0.2084 - val_lwlrap: 0.7871
Epoch 5/60
60/60 - 9s - loss: 0.1084 - lwlrap: 0.8481 - val_loss: 0.1454 - val_lwlrap: 0.7742
Epoch 6/60
60/60 - 10s - loss: 0.0595 - lwlrap: 0.8866 - val_loss: 0.1677 - val_lwlrap: 0.8378
Epoch 7/60
60/60 - 9s - loss: 0.0502 - lwlrap: 0.9407 - val_loss: 0.1992 - val_lwlrap: 0.8687
Epoch 8/60
60/60 - 9s - loss: 0.0167 - lwlrap: 0.9662 - val_loss: 0.1055 - val_lwlrap: 0.8273
Epoch 9/60
60/60 - 9s - loss: 0.0054 - lwlrap: 0.9776 - val_loss: 0.0089 - val_lwlrap: 0.8559
Epoch 10/60
60/60 - 9s - loss: 0.0098 - lwlrap: 0.9897 - val_loss: 0.0169 - val_lwlrap: 0.8726
Epoch 11/60
60/60 - 9s - loss: 0.0083 - lwlrap: 0.9914 - val_loss: 0.0303 - val_lwlrap: 0.8827
Epoch 12/60
60/60 - 9s - loss: 0.0335 - lwlrap: 0.9930 - val_loss: 0.2443 - val_lwlrap: 0.8789
Epoch 13/60
60/60 - 9s - loss: 0.0302 - lwlrap: 0.9913 - val_loss: 0.1546 - val_lwlrap: 0.8413
Epoch 14/60
60/60 - 9s - loss: 0.0063 - lwlrap: 0.9850 - val_loss: 0.2366 - val_lwlrap: 0.8355
Epoch 15/60
60/60 - 9s - loss: 0.0449 - lwlrap: 0.9669 - val_loss: 0.2092 - val_lwlrap: 0.8208
Epoch 16/60
60/60 - 9s - loss: 0.0106 - lwlrap: 0.9819 - val_loss: 0.2602 - val_lwlrap: 0.8208
Epoch 17/60
60/60 - 9s - loss: 0.0327 - lwlrap: 0.9865 - val_loss: 0.0222 - val_lwlrap: 0.8559
Epoch 18/60
60/60 - 9s - loss: 0.0173 - lwlrap: 0.9951 - val_loss: 0.0769 - val_lwlrap: 0.8595
Epoch 19/60
60/60 - 9s - loss: 0.0060 - lwlrap: 0.9936 - val_loss: 0.1067 - val_lwlrap: 0.8776
Epoch 20/60
60/60 - 9s - loss: 0.0028 - lwlrap: 0.9980 - val_loss: 0.1606 - val_lwlrap: 0.8746
Epoch 21/60
60/60 - 9s - loss: 0.0161 - lwlrap: 0.9990 - val_loss: 0.1481 - val_lwlrap: 0.8673
Epoch 22/60
60/60 - 9s - loss: 0.0024 - lwlrap: 0.9959 - val_loss: 0.2900 - val_lwlrap: 0.8682
Epoch 23/60
60/60 - 9s - loss: 0.0019 - lwlrap: 0.9962 - val_loss: 0.0521 - val_lwlrap: 0.8769
Epoch 24/60
60/60 - 9s - loss: 0.0085 - lwlrap: 0.9977 - val_loss: 0.0783 - val_lwlrap: 0.8659
Epoch 25/60
60/60 - 9s - loss: 0.0015 - lwlrap: 0.9988 - val_loss: 0.1698 - val_lwlrap: 0.8608
Epoch 26/60
60/60 - 9s - loss: 0.0032 - lwlrap: 0.9939 - val_loss: 0.0495 - val_lwlrap: 0.8393
Epoch 27/60
60/60 - 9s - loss: 0.0013 - lwlrap: 0.9973 - val_loss: 0.0014 - val_lwlrap: 0.8749
Epoch 28/60
60/60 - 9s - loss: 7.3100e-04 - lwlrap: 0.9983 - val_loss: 0.2810 - val_lwlrap: 0.8625
Epoch 29/60
60/60 - 9s - loss: 0.0049 - lwlrap: 0.9984 - val_loss: 0.0031 - val_lwlrap: 0.8552
Epoch 30/60
60/60 - 9s - loss: 5.6593e-04 - lwlrap: 0.9988 - val_loss: 7.8240e-04 - val_lwlrap: 0.8759
Epoch 31/60
60/60 - 9s - loss: 2.4619e-04 - lwlrap: 0.9992 - val_loss: 0.0017 - val_lwlrap: 0.8847
Epoch 32/60
60/60 - 9s - loss: 0.0138 - lwlrap: 0.9983 - val_loss: 0.0019 - val_lwlrap: 0.8642
Epoch 33/60
60/60 - 9s - loss: 0.0019 - lwlrap: 0.9978 - val_loss: 0.1796 - val_lwlrap: 0.8746
Epoch 34/60
60/60 - 9s - loss: 0.0012 - lwlrap: 0.9985 - val_loss: 0.0738 - val_lwlrap: 0.8913
Epoch 35/60
60/60 - 9s - loss: 0.0056 - lwlrap: 0.9976 - val_loss: 0.0213 - val_lwlrap: 0.8514
Epoch 36/60
60/60 - 9s - loss: 0.0095 - lwlrap: 0.9992 - val_loss: 0.3599 - val_lwlrap: 0.8696
Epoch 37/60
60/60 - 9s - loss: 4.1052e-04 - lwlrap: 0.9991 - val_loss: 0.3231 - val_lwlrap: 0.8612
Epoch 38/60
60/60 - 9s - loss: 0.0022 - lwlrap: 0.9965 - val_loss: 0.2963 - val_lwlrap: 0.8511
Epoch 39/60
60/60 - 9s - loss: 4.4072e-04 - lwlrap: 0.9979 - val_loss: 0.2002 - val_lwlrap: 0.8613
Epoch 40/60
60/60 - 9s - loss: 8.8892e-04 - lwlrap: 0.9995 - val_loss: 0.0628 - val_lwlrap: 0.8569
Epoch 41/60
60/60 - 9s - loss: 9.1377e-04 - lwlrap: 0.9979 - val_loss: 0.0330 - val_lwlrap: 0.8587
Epoch 42/60
60/60 - 9s - loss: 0.0025 - lwlrap: 0.9987 - val_loss: 0.0044 - val_lwlrap: 0.8683
Epoch 43/60
60/60 - 9s - loss: 0.0010 - lwlrap: 0.9993 - val_loss: 0.0752 - val_lwlrap: 0.8719
Epoch 44/60
60/60 - 9s - loss: 6.1758e-04 - lwlrap: 0.9987 - val_loss: 0.0247 - val_lwlrap: 0.8820
Epoch 45/60
60/60 - 9s - loss: 0.0014 - lwlrap: 0.9983 - val_loss: 0.0014 - val_lwlrap: 0.8792
Epoch 46/60
60/60 - 9s - loss: 4.5602e-04 - lwlrap: 0.9983 - val_loss: 0.0074 - val_lwlrap: 0.8701
Epoch 47/60
60/60 - 9s - loss: 7.0957e-04 - lwlrap: 0.9985 - val_loss: 0.0132 - val_lwlrap: 0.8593
Epoch 48/60
60/60 - 9s - loss: 0.0088 - lwlrap: 0.9983 - val_loss: 0.0103 - val_lwlrap: 0.8631
Epoch 49/60
60/60 - 9s - loss: 1.7429e-04 - lwlrap: 0.9990 - val_loss: 0.0044 - val_lwlrap: 0.8584
Epoch 50/60
60/60 - 9s - loss: 2.1993e-04 - lwlrap: 0.9986 - val_loss: 0.0116 - val_lwlrap: 0.8710
Epoch 51/60
60/60 - 9s - loss: 0.0031 - lwlrap: 0.9984 - val_loss: 0.0056 - val_lwlrap: 0.8674
Epoch 52/60
60/60 - 9s - loss: 0.0021 - lwlrap: 0.9988 - val_loss: 0.0096 - val_lwlrap: 0.8632
Epoch 53/60
60/60 - 9s - loss: 2.6919e-04 - lwlrap: 0.9987 - val_loss: 0.0389 - val_lwlrap: 0.8659
Epoch 54/60
60/60 - 9s - loss: 0.0029 - lwlrap: 0.9988 - val_loss: 0.0592 - val_lwlrap: 0.8777

-------------   Fold 5 / 5  -------------

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-28/1/pretrained_best_fold4.h5

 -> Training Model 

Epoch 1/60
61/61 - 11s - loss: 0.1707 - lwlrap: 0.5497 - val_loss: 0.1138 - val_lwlrap: 0.7011
Epoch 2/60
61/61 - 10s - loss: 0.1789 - lwlrap: 0.8592 - val_loss: 0.1146 - val_lwlrap: 0.7155
Epoch 3/60
61/61 - 10s - loss: 0.0607 - lwlrap: 0.8534 - val_loss: 0.1302 - val_lwlrap: 0.6713
Epoch 4/60
61/61 - 9s - loss: 0.1096 - lwlrap: 0.8419 - val_loss: 0.0891 - val_lwlrap: 0.6545
Epoch 5/60
61/61 - 10s - loss: 0.0629 - lwlrap: 0.8692 - val_loss: 0.1078 - val_lwlrap: 0.6383
Epoch 6/60
61/61 - 10s - loss: 0.0354 - lwlrap: 0.8761 - val_loss: 0.1861 - val_lwlrap: 0.6298
Epoch 7/60
61/61 - 10s - loss: 0.0476 - lwlrap: 0.9383 - val_loss: 0.0654 - val_lwlrap: 0.7505
Epoch 8/60
61/61 - 10s - loss: 0.0337 - lwlrap: 0.9563 - val_loss: 0.0454 - val_lwlrap: 0.8235
Epoch 9/60
61/61 - 10s - loss: 0.0205 - lwlrap: 0.9746 - val_loss: 0.0621 - val_lwlrap: 0.8287
Epoch 10/60
61/61 - 10s - loss: 0.0102 - lwlrap: 0.9834 - val_loss: 0.0524 - val_lwlrap: 0.8629
Epoch 11/60
61/61 - 10s - loss: 0.0063 - lwlrap: 0.9858 - val_loss: 0.0506 - val_lwlrap: 0.8709
Epoch 12/60
61/61 - 10s - loss: 0.0045 - lwlrap: 0.9872 - val_loss: 0.0599 - val_lwlrap: 0.8693
Epoch 13/60
61/61 - 10s - loss: 0.0063 - lwlrap: 0.9793 - val_loss: 0.0790 - val_lwlrap: 0.8384
Epoch 14/60
61/61 - 9s - loss: 0.0277 - lwlrap: 0.9737 - val_loss: 0.1039 - val_lwlrap: 0.7916
Epoch 15/60
61/61 - 10s - loss: 0.0259 - lwlrap: 0.9799 - val_loss: 0.1048 - val_lwlrap: 0.7585
Epoch 16/60
61/61 - 10s - loss: 0.0244 - lwlrap: 0.9743 - val_loss: 0.0782 - val_lwlrap: 0.7811
Epoch 17/60
61/61 - 10s - loss: 0.0056 - lwlrap: 0.9895 - val_loss: 0.0808 - val_lwlrap: 0.8449
Epoch 18/60
61/61 - 10s - loss: 0.0061 - lwlrap: 0.9925 - val_loss: 0.0998 - val_lwlrap: 0.8260
Epoch 19/60
61/61 - 9s - loss: 0.0049 - lwlrap: 0.9921 - val_loss: 0.0733 - val_lwlrap: 0.8459
Epoch 20/60
61/61 - 10s - loss: 0.0087 - lwlrap: 0.9945 - val_loss: 0.0504 - val_lwlrap: 0.8637
Epoch 21/60
61/61 - 10s - loss: 0.0019 - lwlrap: 0.9971 - val_loss: 0.0534 - val_lwlrap: 0.8499
Epoch 22/60
61/61 - 10s - loss: 0.0148 - lwlrap: 0.9974 - val_loss: 0.0874 - val_lwlrap: 0.8541
Epoch 23/60
61/61 - 10s - loss: 0.0091 - lwlrap: 0.9961 - val_loss: 0.0334 - val_lwlrap: 0.8703
Epoch 24/60
61/61 - 10s - loss: 0.0017 - lwlrap: 0.9962 - val_loss: 0.0729 - val_lwlrap: 0.8255
Epoch 25/60
61/61 - 10s - loss: 0.0143 - lwlrap: 0.9920 - val_loss: 0.0703 - val_lwlrap: 0.8547
Epoch 26/60
61/61 - 9s - loss: 0.0012 - lwlrap: 0.9943 - val_loss: 0.0774 - val_lwlrap: 0.8449
Epoch 27/60
61/61 - 10s - loss: 0.0035 - lwlrap: 0.9982 - val_loss: 0.0546 - val_lwlrap: 0.8488
Epoch 28/60
61/61 - 10s - loss: 0.0165 - lwlrap: 0.9975 - val_loss: 0.0793 - val_lwlrap: 0.8373
Epoch 29/60
61/61 - 10s - loss: 0.0060 - lwlrap: 0.9981 - val_loss: 0.0730 - val_lwlrap: 0.8472
Epoch 30/60
61/61 - 10s - loss: 6.5407e-04 - lwlrap: 0.9972 - val_loss: 0.1131 - val_lwlrap: 0.8658
Epoch 31/60
61/61 - 10s - loss: 5.4686e-04 - lwlrap: 0.9964 - val_loss: 0.0585 - val_lwlrap: 0.8426
Traceback (most recent call last):


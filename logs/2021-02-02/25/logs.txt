
-------------   Fold 1 / 5  -------------

 -> Using 2285 pseudo labels 

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-31/8/pretrained_best_fold0.h5

 -> Training Model 

Epoch 1/85
30/30 - 10s - loss: 0.1612 - lwlrap: 0.1435 - val_loss: 0.1660 - val_lwlrap: 0.6425
Epoch 2/85
30/30 - 9s - loss: 0.1792 - lwlrap: 0.1906 - val_loss: 0.1610 - val_lwlrap: 0.6637
Epoch 3/85
30/30 - 9s - loss: 0.1015 - lwlrap: 0.1999 - val_loss: 0.1555 - val_lwlrap: 0.6894
Epoch 4/85
30/30 - 8s - loss: 0.1133 - lwlrap: 0.1882 - val_loss: 0.1466 - val_lwlrap: 0.6758
Epoch 5/85
30/30 - 8s - loss: 0.0960 - lwlrap: 0.2029 - val_loss: 0.1529 - val_lwlrap: 0.6778
Epoch 6/85
30/30 - 9s - loss: 0.1094 - lwlrap: 0.2077 - val_loss: 0.1530 - val_lwlrap: 0.7039
Epoch 7/85
30/30 - 9s - loss: 0.0973 - lwlrap: 0.2054 - val_loss: 0.1177 - val_lwlrap: 0.8061
Epoch 8/85
30/30 - 9s - loss: 0.1145 - lwlrap: 0.1961 - val_loss: 0.1364 - val_lwlrap: 0.7244
Epoch 9/85
30/30 - 8s - loss: 0.0429 - lwlrap: 0.2108 - val_loss: 0.1257 - val_lwlrap: 0.7266
Epoch 10/85
30/30 - 8s - loss: 0.0743 - lwlrap: 0.2193 - val_loss: 0.1895 - val_lwlrap: 0.4753
Epoch 11/85
30/30 - 8s - loss: 0.1075 - lwlrap: 0.2039 - val_loss: 0.1231 - val_lwlrap: 0.6716
Epoch 12/85
30/30 - 9s - loss: 0.0389 - lwlrap: 0.2119 - val_loss: 0.0964 - val_lwlrap: 0.8077
Epoch 13/85
30/30 - 9s - loss: 0.0636 - lwlrap: 0.2456 - val_loss: 0.1056 - val_lwlrap: 0.8100
Epoch 14/85
30/30 - 9s - loss: 0.0832 - lwlrap: 0.2247 - val_loss: 0.0952 - val_lwlrap: 0.8201
Epoch 15/85
30/30 - 9s - loss: 0.0461 - lwlrap: 0.1981 - val_loss: 0.0925 - val_lwlrap: 0.8216
Epoch 16/85
30/30 - 8s - loss: 0.0705 - lwlrap: 0.2366 - val_loss: 0.0845 - val_lwlrap: 0.8277
Epoch 17/85
30/30 - 9s - loss: 0.0846 - lwlrap: 0.2058 - val_loss: 0.0931 - val_lwlrap: 0.8350
Epoch 18/85
30/30 - 9s - loss: 0.0534 - lwlrap: 0.2136 - val_loss: 0.0931 - val_lwlrap: 0.8639
Epoch 19/85
30/30 - 9s - loss: 0.0391 - lwlrap: 0.2059 - val_loss: 0.1169 - val_lwlrap: 0.8643
Epoch 20/85
30/30 - 8s - loss: 0.0327 - lwlrap: 0.2022 - val_loss: 0.0949 - val_lwlrap: 0.8578
Epoch 21/85
30/30 - 9s - loss: 0.0302 - lwlrap: 0.2010 - val_loss: 0.0904 - val_lwlrap: 0.8620
Epoch 22/85
30/30 - 8s - loss: 0.0559 - lwlrap: 0.2259 - val_loss: 0.0977 - val_lwlrap: 0.8594
Epoch 23/85
30/30 - 8s - loss: 0.0767 - lwlrap: 0.2517 - val_loss: 0.0980 - val_lwlrap: 0.8553
Epoch 24/85
30/30 - 8s - loss: 0.0429 - lwlrap: 0.2663 - val_loss: 0.1002 - val_lwlrap: 0.8549
Epoch 25/85
30/30 - 9s - loss: 0.0586 - lwlrap: 0.2038 - val_loss: 0.0901 - val_lwlrap: 0.8389
Epoch 26/85
30/30 - 8s - loss: 0.0473 - lwlrap: 0.2167 - val_loss: 0.1007 - val_lwlrap: 0.8503
Epoch 27/85
30/30 - 8s - loss: 0.0623 - lwlrap: 0.1990 - val_loss: 0.0959 - val_lwlrap: 0.8290
Epoch 28/85
30/30 - 8s - loss: 0.0528 - lwlrap: 0.2103 - val_loss: 0.1060 - val_lwlrap: 0.8463
Epoch 29/85
30/30 - 8s - loss: 0.0281 - lwlrap: 0.2163 - val_loss: 0.0853 - val_lwlrap: 0.8337
Epoch 30/85
30/30 - 8s - loss: 0.0326 - lwlrap: 0.2507 - val_loss: 0.0916 - val_lwlrap: 0.8327
Epoch 31/85
30/30 - 8s - loss: 0.0736 - lwlrap: 0.2271 - val_loss: 0.0795 - val_lwlrap: 0.8477
Epoch 32/85
30/30 - 8s - loss: 0.0540 - lwlrap: 0.2277 - val_loss: 0.1151 - val_lwlrap: 0.8338
Epoch 33/85
30/30 - 9s - loss: 0.0755 - lwlrap: 0.1948 - val_loss: 0.0997 - val_lwlrap: 0.8412
Epoch 34/85
30/30 - 8s - loss: 0.0331 - lwlrap: 0.2236 - val_loss: 0.1008 - val_lwlrap: 0.8369
Epoch 35/85
30/30 - 8s - loss: 0.0260 - lwlrap: 0.2230 - val_loss: 0.1097 - val_lwlrap: 0.8388
Epoch 36/85
30/30 - 8s - loss: 0.0370 - lwlrap: 0.2245 - val_loss: 0.1182 - val_lwlrap: 0.8574
Epoch 37/85
30/30 - 8s - loss: 0.0554 - lwlrap: 0.2308 - val_loss: 0.0981 - val_lwlrap: 0.8428
Epoch 38/85
30/30 - 8s - loss: 0.0143 - lwlrap: 0.2202 - val_loss: 0.1212 - val_lwlrap: 0.8335
Epoch 39/85
30/30 - 8s - loss: 0.0105 - lwlrap: 0.2353 - val_loss: 0.1317 - val_lwlrap: 0.8534
Epoch 40/85
30/30 - 8s - loss: 0.0730 - lwlrap: 0.2016 - val_loss: 0.0835 - val_lwlrap: 0.8393
Epoch 41/85
30/30 - 8s - loss: 0.0615 - lwlrap: 0.2582 - val_loss: 0.1159 - val_lwlrap: 0.8383
Epoch 42/85
30/30 - 8s - loss: 0.0481 - lwlrap: 0.2138 - val_loss: 0.0815 - val_lwlrap: 0.8559
Epoch 43/85
30/30 - 8s - loss: 0.0118 - lwlrap: 0.2383 - val_loss: 0.1143 - val_lwlrap: 0.8428
Epoch 44/85
30/30 - 8s - loss: 0.0651 - lwlrap: 0.2202 - val_loss: 0.1176 - val_lwlrap: 0.8540

-------------   Fold 2 / 5  -------------

 -> Using 2508 pseudo labels 

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-31/8/pretrained_best_fold1.h5

 -> Training Model 

Epoch 1/85
30/30 - 10s - loss: 0.1079 - lwlrap: 0.1522 - val_loss: 0.1168 - val_lwlrap: 0.6579
Epoch 2/85
30/30 - 9s - loss: 0.1474 - lwlrap: 0.2062 - val_loss: 0.0863 - val_lwlrap: 0.7721
Epoch 3/85
30/30 - 9s - loss: 0.1075 - lwlrap: 0.2114 - val_loss: 0.0871 - val_lwlrap: 0.8299
Epoch 4/85
30/30 - 8s - loss: 0.1033 - lwlrap: 0.2049 - val_loss: 0.1002 - val_lwlrap: 0.7947
Epoch 5/85
30/30 - 9s - loss: 0.1284 - lwlrap: 0.1937 - val_loss: 0.0646 - val_lwlrap: 0.8693
Epoch 6/85
30/30 - 8s - loss: 0.0823 - lwlrap: 0.2009 - val_loss: 0.0833 - val_lwlrap: 0.7381
Epoch 7/85
30/30 - 8s - loss: 0.1086 - lwlrap: 0.2084 - val_loss: 0.0824 - val_lwlrap: 0.8623
Epoch 8/85
30/30 - 8s - loss: 0.0816 - lwlrap: 0.2102 - val_loss: 0.0769 - val_lwlrap: 0.8607
Epoch 9/85
30/30 - 8s - loss: 0.0437 - lwlrap: 0.2087 - val_loss: 0.0866 - val_lwlrap: 0.8311
Epoch 10/85
30/30 - 9s - loss: 0.0851 - lwlrap: 0.2061 - val_loss: 0.0573 - val_lwlrap: 0.8765
Epoch 11/85
30/30 - 9s - loss: 0.0556 - lwlrap: 0.2104 - val_loss: 0.0901 - val_lwlrap: 0.8795
Epoch 12/85
30/30 - 8s - loss: 0.0836 - lwlrap: 0.2391 - val_loss: 0.0451 - val_lwlrap: 0.8672
Epoch 13/85
30/30 - 8s - loss: 0.0886 - lwlrap: 0.2408 - val_loss: 0.0463 - val_lwlrap: 0.8630
Epoch 14/85
30/30 - 8s - loss: 0.0662 - lwlrap: 0.2071 - val_loss: 0.0548 - val_lwlrap: 0.8758
Epoch 15/85
30/30 - 8s - loss: 0.0157 - lwlrap: 0.2351 - val_loss: 0.0438 - val_lwlrap: 0.8933
Epoch 16/85
30/30 - 9s - loss: 0.0774 - lwlrap: 0.2212 - val_loss: 0.0266 - val_lwlrap: 0.9178
Epoch 17/85
30/30 - 8s - loss: 0.0468 - lwlrap: 0.2259 - val_loss: 0.0329 - val_lwlrap: 0.9033
Epoch 18/85
30/30 - 8s - loss: 0.0294 - lwlrap: 0.2700 - val_loss: 0.0406 - val_lwlrap: 0.9148
Epoch 19/85
30/30 - 8s - loss: 0.0610 - lwlrap: 0.2153 - val_loss: 0.0420 - val_lwlrap: 0.9056
Epoch 20/85
30/30 - 8s - loss: 0.0694 - lwlrap: 0.2247 - val_loss: 0.0392 - val_lwlrap: 0.9047
Epoch 21/85
30/30 - 8s - loss: 0.0651 - lwlrap: 0.2288 - val_loss: 0.0509 - val_lwlrap: 0.9022
Epoch 22/85
30/30 - 8s - loss: 0.0697 - lwlrap: 0.2237 - val_loss: 0.0531 - val_lwlrap: 0.9058
Epoch 23/85
30/30 - 8s - loss: 0.0671 - lwlrap: 0.2517 - val_loss: 0.0467 - val_lwlrap: 0.9083
Epoch 24/85
30/30 - 8s - loss: 0.0122 - lwlrap: 0.2589 - val_loss: 0.0357 - val_lwlrap: 0.9105
Epoch 25/85
30/30 - 8s - loss: 0.0667 - lwlrap: 0.2049 - val_loss: 0.0405 - val_lwlrap: 0.8884
Epoch 26/85
30/30 - 8s - loss: 0.0215 - lwlrap: 0.2287 - val_loss: 0.0635 - val_lwlrap: 0.8977
Epoch 27/85
30/30 - 8s - loss: 0.0429 - lwlrap: 0.1981 - val_loss: 0.0562 - val_lwlrap: 0.8761
Epoch 28/85
30/30 - 8s - loss: 0.0621 - lwlrap: 0.2230 - val_loss: 0.0384 - val_lwlrap: 0.9044
Epoch 29/85
30/30 - 8s - loss: 0.0226 - lwlrap: 0.2460 - val_loss: 0.0491 - val_lwlrap: 0.8953
Epoch 30/85
30/30 - 8s - loss: 0.0543 - lwlrap: 0.2399 - val_loss: 0.0590 - val_lwlrap: 0.9020
Epoch 31/85
30/30 - 8s - loss: 0.0511 - lwlrap: 0.2105 - val_loss: 0.0388 - val_lwlrap: 0.8742
Epoch 32/85
30/30 - 8s - loss: 0.0593 - lwlrap: 0.2102 - val_loss: 0.0551 - val_lwlrap: 0.8819
Epoch 33/85
30/30 - 8s - loss: 0.0595 - lwlrap: 0.2706 - val_loss: 0.0622 - val_lwlrap: 0.8848
Epoch 34/85
30/30 - 8s - loss: 0.0204 - lwlrap: 0.2083 - val_loss: 0.0682 - val_lwlrap: 0.8976
Epoch 35/85
30/30 - 8s - loss: 0.0590 - lwlrap: 0.2096 - val_loss: 0.0500 - val_lwlrap: 0.9006
Epoch 36/85
30/30 - 8s - loss: 0.0652 - lwlrap: 0.2214 - val_loss: 0.0487 - val_lwlrap: 0.8830
Epoch 37/85
30/30 - 8s - loss: 0.0691 - lwlrap: 0.2217 - val_loss: 0.0375 - val_lwlrap: 0.9014
Epoch 38/85
30/30 - 8s - loss: 0.0469 - lwlrap: 0.2421 - val_loss: 0.0419 - val_lwlrap: 0.9059
Epoch 39/85
30/30 - 8s - loss: 0.0134 - lwlrap: 0.2209 - val_loss: 0.0433 - val_lwlrap: 0.9077
Epoch 40/85
30/30 - 8s - loss: 0.0030 - lwlrap: 0.2250 - val_loss: 0.0464 - val_lwlrap: 0.8996
Epoch 41/85
30/30 - 8s - loss: 0.0556 - lwlrap: 0.2282 - val_loss: 0.0539 - val_lwlrap: 0.9069

-------------   Fold 3 / 5  -------------

 -> Using 2332 pseudo labels 

 -> Preparing Data 


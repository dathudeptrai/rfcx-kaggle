
-------------   Fold 1 / 5  -------------

 -> Using 2727 pseudo labels 

 -> Preparing Data 

 -> Preparing Model 

 -> Training Model 

Epoch 1/85
30/30 - 10s - loss: 0.1861 - lwlrap: 0.0789 - val_loss: 0.1957 - val_lwlrap: 0.1715
Epoch 2/85
30/30 - 9s - loss: 0.1696 - lwlrap: 0.1167 - val_loss: 0.1972 - val_lwlrap: 0.2040
Epoch 3/85
30/30 - 8s - loss: 0.1804 - lwlrap: 0.1354 - val_loss: 0.1887 - val_lwlrap: 0.3073
Epoch 4/85
30/30 - 9s - loss: 0.1504 - lwlrap: 0.1566 - val_loss: 0.2042 - val_lwlrap: 0.3424
Epoch 5/85
30/30 - 8s - loss: 0.2719 - lwlrap: 0.1810 - val_loss: 0.1911 - val_lwlrap: 0.4364
Epoch 6/85
30/30 - 9s - loss: 0.2400 - lwlrap: 0.1691 - val_loss: 0.1780 - val_lwlrap: 0.5277
Epoch 7/85
30/30 - 9s - loss: 0.2004 - lwlrap: 0.1887 - val_loss: 0.1666 - val_lwlrap: 0.5349
Epoch 8/85
30/30 - 9s - loss: 0.0969 - lwlrap: 0.1781 - val_loss: 0.1749 - val_lwlrap: 0.5857
Epoch 9/85
30/30 - 9s - loss: 0.1050 - lwlrap: 0.1893 - val_loss: 0.1692 - val_lwlrap: 0.6078
Epoch 10/85
30/30 - 9s - loss: 0.1227 - lwlrap: 0.1942 - val_loss: 0.1356 - val_lwlrap: 0.6981
Epoch 11/85
30/30 - 9s - loss: 0.1674 - lwlrap: 0.1821 - val_loss: 0.1452 - val_lwlrap: 0.7695
Epoch 12/85
30/30 - 8s - loss: 0.1865 - lwlrap: 0.2135 - val_loss: 0.1380 - val_lwlrap: 0.7647
Epoch 13/85
30/30 - 9s - loss: 0.1232 - lwlrap: 0.2028 - val_loss: 0.1286 - val_lwlrap: 0.8090
Epoch 14/85
30/30 - 8s - loss: 0.0748 - lwlrap: 0.1948 - val_loss: 0.1384 - val_lwlrap: 0.7804
Epoch 15/85
30/30 - 9s - loss: 0.0339 - lwlrap: 0.2059 - val_loss: 0.1160 - val_lwlrap: 0.8422
Epoch 16/85
30/30 - 8s - loss: 0.0590 - lwlrap: 0.1992 - val_loss: 0.1159 - val_lwlrap: 0.8283
Epoch 17/85
30/30 - 8s - loss: 0.0492 - lwlrap: 0.2017 - val_loss: 0.0981 - val_lwlrap: 0.8404
Epoch 18/85
30/30 - 9s - loss: 0.0652 - lwlrap: 0.2024 - val_loss: 0.0965 - val_lwlrap: 0.8646
Epoch 19/85
30/30 - 8s - loss: 0.0211 - lwlrap: 0.2143 - val_loss: 0.0896 - val_lwlrap: 0.8575
Epoch 20/85
30/30 - 8s - loss: 0.0277 - lwlrap: 0.2313 - val_loss: 0.1118 - val_lwlrap: 0.8545
Epoch 21/85
30/30 - 8s - loss: 0.0470 - lwlrap: 0.2644 - val_loss: 0.0761 - val_lwlrap: 0.8491
Epoch 22/85
30/30 - 8s - loss: 0.0496 - lwlrap: 0.2174 - val_loss: 0.0966 - val_lwlrap: 0.8525
Epoch 23/85
30/30 - 8s - loss: 0.0689 - lwlrap: 0.2136 - val_loss: 0.1064 - val_lwlrap: 0.8414
Epoch 24/85
30/30 - 8s - loss: 0.0425 - lwlrap: 0.2200 - val_loss: 0.0953 - val_lwlrap: 0.8506
Epoch 25/85
30/30 - 8s - loss: 0.0453 - lwlrap: 0.2211 - val_loss: 0.1074 - val_lwlrap: 0.8514
Epoch 26/85
30/30 - 8s - loss: 0.0743 - lwlrap: 0.2099 - val_loss: 0.0712 - val_lwlrap: 0.8404
Epoch 27/85
30/30 - 8s - loss: 0.0539 - lwlrap: 0.2132 - val_loss: 0.0867 - val_lwlrap: 0.8478
Epoch 28/85
30/30 - 8s - loss: 0.0522 - lwlrap: 0.2245 - val_loss: 0.1098 - val_lwlrap: 0.8479
Epoch 29/85
30/30 - 9s - loss: 0.0601 - lwlrap: 0.2194 - val_loss: 0.0699 - val_lwlrap: 0.8658
Epoch 30/85
30/30 - 8s - loss: 0.0341 - lwlrap: 0.2145 - val_loss: 0.1175 - val_lwlrap: 0.8349
Epoch 31/85
30/30 - 8s - loss: 0.0605 - lwlrap: 0.2123 - val_loss: 0.1010 - val_lwlrap: 0.8399
Epoch 32/85
30/30 - 8s - loss: 0.0864 - lwlrap: 0.2263 - val_loss: 0.0846 - val_lwlrap: 0.8503
Epoch 33/85
30/30 - 8s - loss: 0.0546 - lwlrap: 0.2140 - val_loss: 0.0855 - val_lwlrap: 0.8449
Epoch 34/85
30/30 - 9s - loss: 0.0241 - lwlrap: 0.2394 - val_loss: 0.0711 - val_lwlrap: 0.8669
Epoch 35/85
30/30 - 8s - loss: 0.0295 - lwlrap: 0.2576 - val_loss: 0.0813 - val_lwlrap: 0.8475
Epoch 36/85
30/30 - 8s - loss: 0.0587 - lwlrap: 0.2191 - val_loss: 0.0948 - val_lwlrap: 0.8498
Epoch 37/85
30/30 - 8s - loss: 0.0434 - lwlrap: 0.2139 - val_loss: 0.0947 - val_lwlrap: 0.8516
Epoch 38/85
30/30 - 8s - loss: 0.0426 - lwlrap: 0.2322 - val_loss: 0.0938 - val_lwlrap: 0.8505
Epoch 39/85
30/30 - 8s - loss: 0.0138 - lwlrap: 0.2369 - val_loss: 0.0806 - val_lwlrap: 0.8619
Epoch 40/85
30/30 - 8s - loss: 0.0097 - lwlrap: 0.2212 - val_loss: 0.0775 - val_lwlrap: 0.8650
Epoch 41/85
30/30 - 8s - loss: 0.0120 - lwlrap: 0.2099 - val_loss: 0.0782 - val_lwlrap: 0.8461
Epoch 42/85
30/30 - 8s - loss: 0.0438 - lwlrap: 0.2394 - val_loss: 0.0766 - val_lwlrap: 0.8596
Epoch 43/85
30/30 - 9s - loss: 0.0097 - lwlrap: 0.2271 - val_loss: 0.0947 - val_lwlrap: 0.8736
Epoch 44/85
30/30 - 8s - loss: 0.0052 - lwlrap: 0.2092 - val_loss: 0.0765 - val_lwlrap: 0.8663
Epoch 45/85
30/30 - 8s - loss: 0.0070 - lwlrap: 0.2039 - val_loss: 0.0951 - val_lwlrap: 0.8510
Epoch 46/85
30/30 - 8s - loss: 0.0389 - lwlrap: 0.2420 - val_loss: 0.1087 - val_lwlrap: 0.8505
Epoch 47/85
30/30 - 8s - loss: 0.0173 - lwlrap: 0.2075 - val_loss: 0.0741 - val_lwlrap: 0.8561
Epoch 48/85
30/30 - 8s - loss: 0.0065 - lwlrap: 0.2155 - val_loss: 0.0866 - val_lwlrap: 0.8614
Epoch 49/85
30/30 - 8s - loss: 0.0331 - lwlrap: 0.2153 - val_loss: 0.0790 - val_lwlrap: 0.8616
Epoch 50/85
30/30 - 8s - loss: 0.0079 - lwlrap: 0.2065 - val_loss: 0.1034 - val_lwlrap: 0.8607
Epoch 51/85
30/30 - 8s - loss: 0.0463 - lwlrap: 0.2025 - val_loss: 0.1171 - val_lwlrap: 0.8503
Epoch 52/85
30/30 - 8s - loss: 0.0584 - lwlrap: 0.2083 - val_loss: 0.1068 - val_lwlrap: 0.8415
Epoch 53/85
30/30 - 8s - loss: 0.0431 - lwlrap: 0.2155 - val_loss: 0.0808 - val_lwlrap: 0.8651
Epoch 54/85
30/30 - 8s - loss: 0.0582 - lwlrap: 0.2015 - val_loss: 0.0934 - val_lwlrap: 0.8368
Epoch 55/85
30/30 - 8s - loss: 0.0039 - lwlrap: 0.2207 - val_loss: 0.1126 - val_lwlrap: 0.8367
Epoch 56/85
30/30 - 8s - loss: 0.0041 - lwlrap: 0.2216 - val_loss: 0.1002 - val_lwlrap: 0.8682
Epoch 57/85
30/30 - 8s - loss: 0.0043 - lwlrap: 0.2310 - val_loss: 0.1116 - val_lwlrap: 0.8664
Epoch 58/85
30/30 - 8s - loss: 0.0033 - lwlrap: 0.2455 - val_loss: 0.1016 - val_lwlrap: 0.8619
Epoch 59/85
30/30 - 8s - loss: 0.0062 - lwlrap: 0.2156 - val_loss: 0.0937 - val_lwlrap: 0.8670
Epoch 60/85
30/30 - 8s - loss: 0.0029 - lwlrap: 0.2217 - val_loss: 0.0999 - val_lwlrap: 0.8609
Epoch 61/85
30/30 - 8s - loss: 0.0401 - lwlrap: 0.2382 - val_loss: 0.0865 - val_lwlrap: 0.8535
Epoch 62/85
30/30 - 8s - loss: 0.0407 - lwlrap: 0.2526 - val_loss: 0.0955 - val_lwlrap: 0.8682
Epoch 63/85
30/30 - 8s - loss: 0.0023 - lwlrap: 0.1999 - val_loss: 0.0894 - val_lwlrap: 0.8611

-------------   Fold 2 / 5  -------------

 -> Using 2921 pseudo labels 

 -> Preparing Data 

 -> Preparing Model 

 -> Training Model 

Epoch 1/85
30/30 - 10s - loss: 0.2802 - lwlrap: 0.0725 - val_loss: 0.1774 - val_lwlrap: 0.1753
Epoch 2/85
30/30 - 9s - loss: 0.2726 - lwlrap: 0.1158 - val_loss: 0.1718 - val_lwlrap: 0.2566
Epoch 3/85
30/30 - 9s - loss: 0.1374 - lwlrap: 0.1460 - val_loss: 0.1642 - val_lwlrap: 0.3262
Epoch 4/85
30/30 - 9s - loss: 0.1221 - lwlrap: 0.1778 - val_loss: 0.1432 - val_lwlrap: 0.5056
Epoch 5/85
30/30 - 9s - loss: 0.1146 - lwlrap: 0.1876 - val_loss: 0.1149 - val_lwlrap: 0.6044
Epoch 6/85
30/30 - 9s - loss: 0.1309 - lwlrap: 0.1826 - val_loss: 0.0927 - val_lwlrap: 0.6892
Epoch 7/85
30/30 - 8s - loss: 0.1012 - lwlrap: 0.2182 - val_loss: 0.1045 - val_lwlrap: 0.6813
Epoch 8/85
30/30 - 8s - loss: 0.0947 - lwlrap: 0.2493 - val_loss: 0.0746 - val_lwlrap: 0.7437
Epoch 9/85
30/30 - 8s - loss: 0.1561 - lwlrap: 0.2057 - val_loss: 0.0762 - val_lwlrap: 0.7329
Epoch 10/85
30/30 - 9s - loss: 0.1267 - lwlrap: 0.1956 - val_loss: 0.0820 - val_lwlrap: 0.8096
Epoch 11/85
30/30 - 8s - loss: 0.0780 - lwlrap: 0.1938 - val_loss: 0.0704 - val_lwlrap: 0.7754
Epoch 12/85
30/30 - 8s - loss: 0.1693 - lwlrap: 0.2293 - val_loss: 0.0513 - val_lwlrap: 0.8905
Epoch 13/85
30/30 - 8s - loss: 0.1143 - lwlrap: 0.2240 - val_loss: 0.0542 - val_lwlrap: 0.8718
Epoch 14/85
30/30 - 9s - loss: 0.0863 - lwlrap: 0.2267 - val_loss: 0.0508 - val_lwlrap: 0.8973
Epoch 15/85
30/30 - 9s - loss: 0.0750 - lwlrap: 0.2185 - val_loss: 0.0631 - val_lwlrap: 0.8976
Epoch 16/85
30/30 - 8s - loss: 0.0399 - lwlrap: 0.2330 - val_loss: 0.0519 - val_lwlrap: 0.8765
Epoch 17/85
30/30 - 8s - loss: 0.0470 - lwlrap: 0.2277 - val_loss: 0.0533 - val_lwlrap: 0.8928
Epoch 18/85
30/30 - 9s - loss: 0.0650 - lwlrap: 0.2058 - val_loss: 0.0426 - val_lwlrap: 0.9000
Epoch 19/85
30/30 - 9s - loss: 0.0419 - lwlrap: 0.2289 - val_loss: 0.0339 - val_lwlrap: 0.9044
Epoch 20/85
30/30 - 9s - loss: 0.0433 - lwlrap: 0.2237 - val_loss: 0.0373 - val_lwlrap: 0.9162
Epoch 21/85
30/30 - 8s - loss: 0.0317 - lwlrap: 0.2203 - val_loss: 0.0388 - val_lwlrap: 0.9149
Epoch 22/85
30/30 - 8s - loss: 0.0283 - lwlrap: 0.2326 - val_loss: 0.0409 - val_lwlrap: 0.9126
Epoch 23/85
30/30 - 8s - loss: 0.0605 - lwlrap: 0.1886 - val_loss: 0.0418 - val_lwlrap: 0.9041
Epoch 24/85
30/30 - 8s - loss: 0.0406 - lwlrap: 0.2290 - val_loss: 0.0563 - val_lwlrap: 0.8898
Epoch 25/85
30/30 - 8s - loss: 0.0656 - lwlrap: 0.2182 - val_loss: 0.0495 - val_lwlrap: 0.9044
Epoch 26/85
30/30 - 8s - loss: 0.0642 - lwlrap: 0.2120 - val_loss: 0.0331 - val_lwlrap: 0.9083
Epoch 27/85
30/30 - 8s - loss: 0.0346 - lwlrap: 0.2442 - val_loss: 0.0832 - val_lwlrap: 0.8883
Epoch 28/85
30/30 - 8s - loss: 0.0175 - lwlrap: 0.2353 - val_loss: 0.0605 - val_lwlrap: 0.8892
Epoch 29/85
30/30 - 8s - loss: 0.0468 - lwlrap: 0.2212 - val_loss: 0.0591 - val_lwlrap: 0.8913
Epoch 30/85
30/30 - 8s - loss: 0.0645 - lwlrap: 0.2087 - val_loss: 0.0482 - val_lwlrap: 0.8922
Epoch 31/85
30/30 - 8s - loss: 0.0282 - lwlrap: 0.2686 - val_loss: 0.0299 - val_lwlrap: 0.9098
Epoch 32/85
30/30 - 8s - loss: 0.0621 - lwlrap: 0.2207 - val_loss: 0.0483 - val_lwlrap: 0.8869
Epoch 33/85
30/30 - 8s - loss: 0.0487 - lwlrap: 0.2467 - val_loss: 0.0701 - val_lwlrap: 0.8920
Epoch 34/85
30/30 - 8s - loss: 0.0614 - lwlrap: 0.2617 - val_loss: 0.0502 - val_lwlrap: 0.9112
Epoch 35/85
30/30 - 8s - loss: 0.0557 - lwlrap: 0.2346 - val_loss: 0.0527 - val_lwlrap: 0.8972
Epoch 36/85
30/30 - 8s - loss: 0.0693 - lwlrap: 0.2298 - val_loss: 0.0566 - val_lwlrap: 0.9050
Epoch 37/85
30/30 - 8s - loss: 0.0126 - lwlrap: 0.2503 - val_loss: 0.0441 - val_lwlrap: 0.8871
Epoch 38/85
30/30 - 8s - loss: 0.0488 - lwlrap: 0.2117 - val_loss: 0.0512 - val_lwlrap: 0.8993
Epoch 39/85
30/30 - 8s - loss: 0.0793 - lwlrap: 0.2725 - val_loss: 0.0550 - val_lwlrap: 0.9050
Epoch 40/85
30/30 - 8s - loss: 0.0666 - lwlrap: 0.2031 - val_loss: 0.0646 - val_lwlrap: 0.8992

-------------   Fold 3 / 5  -------------

 -> Using 2900 pseudo labels 

 -> Preparing Data 


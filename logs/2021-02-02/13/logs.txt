
-------------   Fold 1 / 5  -------------

 -> Using 2727 pseudo labels 

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-31/8/pretrained_best_fold0.h5

 -> Training Model 

Epoch 1/85
30/30 - 10s - loss: 0.1246 - lwlrap: 0.1538 - val_loss: 0.1604 - val_lwlrap: 0.6197
Epoch 2/85
30/30 - 9s - loss: 0.1064 - lwlrap: 0.1879 - val_loss: 0.1462 - val_lwlrap: 0.6839
Epoch 3/85
30/30 - 9s - loss: 0.1286 - lwlrap: 0.1895 - val_loss: 0.1400 - val_lwlrap: 0.7111
Epoch 4/85
30/30 - 9s - loss: 0.0924 - lwlrap: 0.2015 - val_loss: 0.1668 - val_lwlrap: 0.7210
Epoch 5/85
30/30 - 8s - loss: 0.1098 - lwlrap: 0.1963 - val_loss: 0.1397 - val_lwlrap: 0.7385
Epoch 6/85
30/30 - 9s - loss: 0.0926 - lwlrap: 0.2016 - val_loss: 0.1408 - val_lwlrap: 0.7810
Epoch 7/85
30/30 - 8s - loss: 0.0964 - lwlrap: 0.2002 - val_loss: 0.1172 - val_lwlrap: 0.7687
Epoch 8/85
30/30 - 8s - loss: 0.0848 - lwlrap: 0.1991 - val_loss: 0.1466 - val_lwlrap: 0.7361
Epoch 9/85
30/30 - 9s - loss: 0.0571 - lwlrap: 0.2013 - val_loss: 0.1487 - val_lwlrap: 0.8129
Epoch 10/85
30/30 - 8s - loss: 0.1273 - lwlrap: 0.2018 - val_loss: 0.1353 - val_lwlrap: 0.7672
Epoch 11/85
30/30 - 8s - loss: 0.0636 - lwlrap: 0.2063 - val_loss: 0.1145 - val_lwlrap: 0.7846
Epoch 12/85
30/30 - 8s - loss: 0.0606 - lwlrap: 0.2083 - val_loss: 0.0950 - val_lwlrap: 0.7892
Epoch 13/85
30/30 - 8s - loss: 0.0588 - lwlrap: 0.2070 - val_loss: 0.1057 - val_lwlrap: 0.7963
Epoch 14/85
30/30 - 8s - loss: 0.0559 - lwlrap: 0.2071 - val_loss: 0.1238 - val_lwlrap: 0.8059
Epoch 15/85
30/30 - 9s - loss: 0.0480 - lwlrap: 0.2121 - val_loss: 0.1103 - val_lwlrap: 0.8246
Epoch 16/85
30/30 - 9s - loss: 0.0569 - lwlrap: 0.2129 - val_loss: 0.0900 - val_lwlrap: 0.8339
Epoch 17/85
30/30 - 9s - loss: 0.0327 - lwlrap: 0.2133 - val_loss: 0.1178 - val_lwlrap: 0.8399
Epoch 18/85
30/30 - 9s - loss: 0.0367 - lwlrap: 0.2161 - val_loss: 0.1065 - val_lwlrap: 0.8492
Epoch 19/85
30/30 - 9s - loss: 0.0339 - lwlrap: 0.2179 - val_loss: 0.1119 - val_lwlrap: 0.8590
Epoch 20/85
30/30 - 8s - loss: 0.0252 - lwlrap: 0.2169 - val_loss: 0.0934 - val_lwlrap: 0.8581
Epoch 21/85
30/30 - 9s - loss: 0.0332 - lwlrap: 0.2119 - val_loss: 0.0814 - val_lwlrap: 0.8614
Epoch 22/85
30/30 - 8s - loss: 0.0435 - lwlrap: 0.2138 - val_loss: 0.0845 - val_lwlrap: 0.8438
Epoch 23/85
30/30 - 8s - loss: 0.0310 - lwlrap: 0.2111 - val_loss: 0.0848 - val_lwlrap: 0.8608
Epoch 24/85
30/30 - 8s - loss: 0.0330 - lwlrap: 0.2180 - val_loss: 0.0903 - val_lwlrap: 0.8395
Epoch 25/85
30/30 - 8s - loss: 0.0545 - lwlrap: 0.2122 - val_loss: 0.0872 - val_lwlrap: 0.8492
Epoch 26/85
30/30 - 8s - loss: 0.0542 - lwlrap: 0.2161 - val_loss: 0.1045 - val_lwlrap: 0.8691
Epoch 27/85
30/30 - 8s - loss: 0.0411 - lwlrap: 0.2218 - val_loss: 0.0867 - val_lwlrap: 0.8442
Epoch 28/85
30/30 - 8s - loss: 0.0392 - lwlrap: 0.2142 - val_loss: 0.1157 - val_lwlrap: 0.8504
Epoch 29/85
30/30 - 8s - loss: 0.0416 - lwlrap: 0.2127 - val_loss: 0.1040 - val_lwlrap: 0.8321
Epoch 30/85
30/30 - 8s - loss: 0.0426 - lwlrap: 0.2126 - val_loss: 0.0819 - val_lwlrap: 0.8552
Epoch 31/85
30/30 - 8s - loss: 0.0353 - lwlrap: 0.2152 - val_loss: 0.1008 - val_lwlrap: 0.8544
Epoch 32/85
30/30 - 8s - loss: 0.0441 - lwlrap: 0.2116 - val_loss: 0.0966 - val_lwlrap: 0.8310
Epoch 33/85
30/30 - 8s - loss: 0.0639 - lwlrap: 0.2143 - val_loss: 0.0528 - val_lwlrap: 0.8470
Epoch 34/85
30/30 - 8s - loss: 0.0285 - lwlrap: 0.2127 - val_loss: 0.0873 - val_lwlrap: 0.8585
Epoch 35/85
30/30 - 8s - loss: 0.0292 - lwlrap: 0.2154 - val_loss: 0.0791 - val_lwlrap: 0.8453
Epoch 36/85
30/30 - 8s - loss: 0.0310 - lwlrap: 0.2197 - val_loss: 0.0714 - val_lwlrap: 0.8625
Epoch 37/85
30/30 - 8s - loss: 0.0357 - lwlrap: 0.2147 - val_loss: 0.0868 - val_lwlrap: 0.8478
Epoch 38/85
30/30 - 8s - loss: 0.0402 - lwlrap: 0.2147 - val_loss: 0.0772 - val_lwlrap: 0.8463
Epoch 39/85
30/30 - 8s - loss: 0.0343 - lwlrap: 0.2172 - val_loss: 0.0834 - val_lwlrap: 0.8556
Epoch 40/85
30/30 - 8s - loss: 0.0269 - lwlrap: 0.2150 - val_loss: 0.1001 - val_lwlrap: 0.8418
Epoch 41/85
30/30 - 8s - loss: 0.0274 - lwlrap: 0.2251 - val_loss: 0.1120 - val_lwlrap: 0.8614
Epoch 42/85
30/30 - 8s - loss: 0.0431 - lwlrap: 0.2172 - val_loss: 0.0864 - val_lwlrap: 0.8535
Epoch 43/85
30/30 - 8s - loss: 0.0294 - lwlrap: 0.2165 - val_loss: 0.0980 - val_lwlrap: 0.8384
Epoch 44/85
30/30 - 8s - loss: 0.0401 - lwlrap: 0.2137 - val_loss: 0.0994 - val_lwlrap: 0.8432
Epoch 45/85
30/30 - 8s - loss: 0.0277 - lwlrap: 0.2174 - val_loss: 0.1163 - val_lwlrap: 0.8422
Epoch 46/85
30/30 - 8s - loss: 0.0422 - lwlrap: 0.2146 - val_loss: 0.1193 - val_lwlrap: 0.8449

-------------   Fold 2 / 5  -------------

 -> Using 2921 pseudo labels 

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-31/8/pretrained_best_fold1.h5

 -> Training Model 

Epoch 1/85
30/30 - 10s - loss: 0.1419 - lwlrap: 0.1527 - val_loss: 0.1036 - val_lwlrap: 0.7134
Epoch 2/85
30/30 - 8s - loss: 0.1169 - lwlrap: 0.2053 - val_loss: 0.0807 - val_lwlrap: 0.8182
Epoch 3/85
30/30 - 9s - loss: 0.0856 - lwlrap: 0.2126 - val_loss: 0.0766 - val_lwlrap: 0.8784
Epoch 4/85
30/30 - 8s - loss: 0.0924 - lwlrap: 0.2152 - val_loss: 0.0728 - val_lwlrap: 0.8762
Epoch 5/85
30/30 - 8s - loss: 0.0646 - lwlrap: 0.2154 - val_loss: 0.0739 - val_lwlrap: 0.8570
Epoch 6/85
30/30 - 8s - loss: 0.0678 - lwlrap: 0.2121 - val_loss: 0.0735 - val_lwlrap: 0.8416
Epoch 7/85
30/30 - 9s - loss: 0.0778 - lwlrap: 0.2134 - val_loss: 0.0672 - val_lwlrap: 0.8853
Epoch 8/85
30/30 - 8s - loss: 0.0693 - lwlrap: 0.2124 - val_loss: 0.0710 - val_lwlrap: 0.8509
Epoch 9/85
30/30 - 8s - loss: 0.1076 - lwlrap: 0.2151 - val_loss: 0.0698 - val_lwlrap: 0.7425
Epoch 10/85
30/30 - 8s - loss: 0.0708 - lwlrap: 0.2135 - val_loss: 0.0840 - val_lwlrap: 0.7809
Epoch 11/85
30/30 - 8s - loss: 0.0703 - lwlrap: 0.2138 - val_loss: 0.0337 - val_lwlrap: 0.8538
Epoch 12/85
30/30 - 8s - loss: 0.0754 - lwlrap: 0.2173 - val_loss: 0.0459 - val_lwlrap: 0.8722
Epoch 13/85
30/30 - 9s - loss: 0.0586 - lwlrap: 0.2228 - val_loss: 0.0414 - val_lwlrap: 0.8978
Epoch 14/85
30/30 - 8s - loss: 0.0370 - lwlrap: 0.2151 - val_loss: 0.0537 - val_lwlrap: 0.8753
Epoch 15/85
30/30 - 8s - loss: 0.0413 - lwlrap: 0.2193 - val_loss: 0.0497 - val_lwlrap: 0.8808
Epoch 16/85
30/30 - 9s - loss: 0.0387 - lwlrap: 0.2193 - val_loss: 0.0355 - val_lwlrap: 0.9094
Epoch 17/85
30/30 - 8s - loss: 0.0499 - lwlrap: 0.2227 - val_loss: 0.0675 - val_lwlrap: 0.8865
Epoch 18/85
30/30 - 8s - loss: 0.0416 - lwlrap: 0.2221 - val_loss: 0.0514 - val_lwlrap: 0.9032
Epoch 19/85
30/30 - 8s - loss: 0.0405 - lwlrap: 0.2229 - val_loss: 0.0457 - val_lwlrap: 0.8993
Epoch 20/85
30/30 - 8s - loss: 0.0366 - lwlrap: 0.2209 - val_loss: 0.0477 - val_lwlrap: 0.8962
Epoch 21/85
30/30 - 8s - loss: 0.0347 - lwlrap: 0.2253 - val_loss: 0.0584 - val_lwlrap: 0.9071
Epoch 22/85
30/30 - 8s - loss: 0.0501 - lwlrap: 0.2255 - val_loss: 0.0506 - val_lwlrap: 0.9082
Epoch 23/85
30/30 - 8s - loss: 0.0662 - lwlrap: 0.2216 - val_loss: 0.0495 - val_lwlrap: 0.8946
Epoch 24/85
30/30 - 8s - loss: 0.0506 - lwlrap: 0.2263 - val_loss: 0.0483 - val_lwlrap: 0.8862
Epoch 25/85
30/30 - 8s - loss: 0.0540 - lwlrap: 0.2249 - val_loss: 0.0540 - val_lwlrap: 0.8891
Epoch 26/85
30/30 - 8s - loss: 0.0467 - lwlrap: 0.2236 - val_loss: 0.0370 - val_lwlrap: 0.9018
Epoch 27/85
30/30 - 8s - loss: 0.0424 - lwlrap: 0.2256 - val_loss: 0.0569 - val_lwlrap: 0.8964
Epoch 28/85
30/30 - 8s - loss: 0.0336 - lwlrap: 0.2231 - val_loss: 0.0605 - val_lwlrap: 0.8954
Epoch 29/85
30/30 - 8s - loss: 0.0385 - lwlrap: 0.2218 - val_loss: 0.0579 - val_lwlrap: 0.8861
Epoch 30/85
30/30 - 8s - loss: 0.0534 - lwlrap: 0.2237 - val_loss: 0.0604 - val_lwlrap: 0.8891
Epoch 31/85
30/30 - 8s - loss: 0.0374 - lwlrap: 0.2272 - val_loss: 0.0430 - val_lwlrap: 0.9010
Epoch 32/85
30/30 - 8s - loss: 0.0302 - lwlrap: 0.2240 - val_loss: 0.0318 - val_lwlrap: 0.8908
Epoch 33/85
30/30 - 8s - loss: 0.0545 - lwlrap: 0.2245 - val_loss: 0.0367 - val_lwlrap: 0.9056
Epoch 34/85
30/30 - 8s - loss: 0.0462 - lwlrap: 0.2330 - val_loss: 0.0516 - val_lwlrap: 0.8728
Epoch 35/85
30/30 - 8s - loss: 0.0459 - lwlrap: 0.2278 - val_loss: 0.0489 - val_lwlrap: 0.8937
Epoch 36/85
30/30 - 8s - loss: 0.0497 - lwlrap: 0.2252 - val_loss: 0.0444 - val_lwlrap: 0.9048

-------------   Fold 3 / 5  -------------

 -> Using 2900 pseudo labels 

 -> Preparing Data 


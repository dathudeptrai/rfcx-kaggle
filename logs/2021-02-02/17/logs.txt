
-------------   Fold 1 / 5  -------------

 -> Using 2727 pseudo labels 

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-31/8/pretrained_best_fold0.h5

 -> Training Model 

Epoch 1/85
30/30 - 10s - loss: 0.0929 - lwlrap: 0.0971 - val_loss: 0.1675 - val_lwlrap: 0.5695
Epoch 2/85
30/30 - 9s - loss: 0.0889 - lwlrap: 0.1439 - val_loss: 0.1674 - val_lwlrap: 0.6492
Epoch 3/85
30/30 - 8s - loss: 0.0941 - lwlrap: 0.1400 - val_loss: 0.1665 - val_lwlrap: 0.7332
Epoch 4/85
30/30 - 9s - loss: 0.1186 - lwlrap: 0.1382 - val_loss: 0.1459 - val_lwlrap: 0.7428
Epoch 5/85
30/30 - 8s - loss: 0.1456 - lwlrap: 0.1228 - val_loss: 0.1521 - val_lwlrap: 0.7045
Epoch 6/85
30/30 - 9s - loss: 0.1574 - lwlrap: 0.1384 - val_loss: 0.1168 - val_lwlrap: 0.7986
Epoch 7/85
30/30 - 8s - loss: 0.1070 - lwlrap: 0.1491 - val_loss: 0.1297 - val_lwlrap: 0.7538
Epoch 8/85
30/30 - 8s - loss: 0.0700 - lwlrap: 0.1188 - val_loss: 0.1704 - val_lwlrap: 0.6464
Epoch 9/85
30/30 - 8s - loss: 0.0827 - lwlrap: 0.1537 - val_loss: 0.1259 - val_lwlrap: 0.7229
Epoch 10/85
30/30 - 8s - loss: 0.0880 - lwlrap: 0.1374 - val_loss: 0.1411 - val_lwlrap: 0.7075
Epoch 11/85
30/30 - 8s - loss: 0.1214 - lwlrap: 0.1464 - val_loss: 0.1116 - val_lwlrap: 0.7696
Epoch 12/85
30/30 - 8s - loss: 0.0833 - lwlrap: 0.1406 - val_loss: 0.0949 - val_lwlrap: 0.7875
Epoch 13/85
30/30 - 8s - loss: 0.1021 - lwlrap: 0.1342 - val_loss: 0.1226 - val_lwlrap: 0.7577
Epoch 14/85
30/30 - 9s - loss: 0.0516 - lwlrap: 0.1211 - val_loss: 0.0972 - val_lwlrap: 0.8209
Epoch 15/85
30/30 - 9s - loss: 0.0545 - lwlrap: 0.1314 - val_loss: 0.1003 - val_lwlrap: 0.8220
Epoch 16/85
30/30 - 8s - loss: 0.0395 - lwlrap: 0.1749 - val_loss: 0.0945 - val_lwlrap: 0.8323
Epoch 17/85
30/30 - 8s - loss: 0.0609 - lwlrap: 0.1684 - val_loss: 0.0918 - val_lwlrap: 0.8279
Epoch 18/85
30/30 - 8s - loss: 0.0229 - lwlrap: 0.1770 - val_loss: 0.0987 - val_lwlrap: 0.8312
Epoch 19/85
30/30 - 9s - loss: 0.0452 - lwlrap: 0.1328 - val_loss: 0.0822 - val_lwlrap: 0.8479
Epoch 20/85
30/30 - 8s - loss: 0.0483 - lwlrap: 0.1470 - val_loss: 0.0939 - val_lwlrap: 0.8392
Epoch 21/85
30/30 - 9s - loss: 0.0439 - lwlrap: 0.1679 - val_loss: 0.0865 - val_lwlrap: 0.8527
Epoch 22/85
30/30 - 9s - loss: 0.0535 - lwlrap: 0.1527 - val_loss: 0.0894 - val_lwlrap: 0.8596
Epoch 23/85
30/30 - 9s - loss: 0.0362 - lwlrap: 0.1352 - val_loss: 0.0906 - val_lwlrap: 0.8611
Epoch 24/85
30/30 - 8s - loss: 0.0611 - lwlrap: 0.1443 - val_loss: 0.0971 - val_lwlrap: 0.8538
Epoch 25/85
30/30 - 8s - loss: 0.0310 - lwlrap: 0.1254 - val_loss: 0.0751 - val_lwlrap: 0.8582
Epoch 26/85
30/30 - 8s - loss: 0.0430 - lwlrap: 0.1691 - val_loss: 0.0930 - val_lwlrap: 0.8369
Epoch 27/85
30/30 - 8s - loss: 0.0433 - lwlrap: 0.1395 - val_loss: 0.0967 - val_lwlrap: 0.8212
Epoch 28/85
30/30 - 8s - loss: 0.0260 - lwlrap: 0.1370 - val_loss: 0.1044 - val_lwlrap: 0.8238
Epoch 29/85
30/30 - 8s - loss: 0.0391 - lwlrap: 0.1210 - val_loss: 0.1170 - val_lwlrap: 0.8118
Epoch 30/85
30/30 - 8s - loss: 0.0562 - lwlrap: 0.1577 - val_loss: 0.0928 - val_lwlrap: 0.8393
Epoch 31/85
30/30 - 8s - loss: 0.0562 - lwlrap: 0.1514 - val_loss: 0.0965 - val_lwlrap: 0.8371
Epoch 32/85
30/30 - 8s - loss: 0.0255 - lwlrap: 0.1814 - val_loss: 0.1091 - val_lwlrap: 0.8187
Epoch 33/85
30/30 - 8s - loss: 0.0343 - lwlrap: 0.1723 - val_loss: 0.1079 - val_lwlrap: 0.8194
Epoch 34/85
30/30 - 8s - loss: 0.0554 - lwlrap: 0.1427 - val_loss: 0.0898 - val_lwlrap: 0.8261
Epoch 35/85
30/30 - 8s - loss: 0.0197 - lwlrap: 0.1450 - val_loss: 0.0958 - val_lwlrap: 0.8386
Epoch 36/85
30/30 - 8s - loss: 0.0127 - lwlrap: 0.1515 - val_loss: 0.0928 - val_lwlrap: 0.8526
Epoch 37/85
30/30 - 8s - loss: 0.0436 - lwlrap: 0.1585 - val_loss: 0.0932 - val_lwlrap: 0.8484
Epoch 38/85
30/30 - 8s - loss: 0.0451 - lwlrap: 0.1846 - val_loss: 0.0893 - val_lwlrap: 0.8569
Epoch 39/85
30/30 - 8s - loss: 0.0161 - lwlrap: 0.1621 - val_loss: 0.0832 - val_lwlrap: 0.8550
Epoch 40/85
30/30 - 8s - loss: 0.0410 - lwlrap: 0.1303 - val_loss: 0.0838 - val_lwlrap: 0.8515
Epoch 41/85
30/30 - 8s - loss: 0.0457 - lwlrap: 0.1314 - val_loss: 0.0883 - val_lwlrap: 0.8550
Epoch 42/85
30/30 - 8s - loss: 0.0433 - lwlrap: 0.1651 - val_loss: 0.0895 - val_lwlrap: 0.8509
Epoch 43/85
30/30 - 8s - loss: 0.0562 - lwlrap: 0.1425 - val_loss: 0.0877 - val_lwlrap: 0.8583
Epoch 44/85
30/30 - 8s - loss: 0.0393 - lwlrap: 0.1307 - val_loss: 0.0912 - val_lwlrap: 0.8364
Epoch 45/85
30/30 - 8s - loss: 0.0456 - lwlrap: 0.1543 - val_loss: 0.0985 - val_lwlrap: 0.8524
Epoch 46/85
30/30 - 8s - loss: 0.0429 - lwlrap: 0.1321 - val_loss: 0.0790 - val_lwlrap: 0.8565
Epoch 47/85
30/30 - 8s - loss: 0.0324 - lwlrap: 0.1613 - val_loss: 0.1158 - val_lwlrap: 0.8514
Epoch 48/85
30/30 - 8s - loss: 0.0132 - lwlrap: 0.1186 - val_loss: 0.0938 - val_lwlrap: 0.8498

-------------   Fold 2 / 5  -------------

 -> Using 2921 pseudo labels 

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-31/8/pretrained_best_fold1.h5

 -> Training Model 

Epoch 1/85
30/30 - 10s - loss: 0.1889 - lwlrap: 0.0922 - val_loss: 0.1184 - val_lwlrap: 0.6463
Epoch 2/85
30/30 - 9s - loss: 0.0849 - lwlrap: 0.1188 - val_loss: 0.1051 - val_lwlrap: 0.7441
Epoch 3/85
30/30 - 9s - loss: 0.0778 - lwlrap: 0.1172 - val_loss: 0.0884 - val_lwlrap: 0.8314
Epoch 4/85
30/30 - 8s - loss: 0.1216 - lwlrap: 0.1301 - val_loss: 0.1038 - val_lwlrap: 0.7272
Epoch 5/85
30/30 - 8s - loss: 0.0786 - lwlrap: 0.1164 - val_loss: 0.0909 - val_lwlrap: 0.8096
Epoch 6/85
30/30 - 8s - loss: 0.0968 - lwlrap: 0.1344 - val_loss: 0.0859 - val_lwlrap: 0.8281
Epoch 7/85
30/30 - 9s - loss: 0.0611 - lwlrap: 0.1307 - val_loss: 0.0766 - val_lwlrap: 0.8339
Epoch 8/85
30/30 - 8s - loss: 0.0730 - lwlrap: 0.1300 - val_loss: 0.0697 - val_lwlrap: 0.8142
Epoch 9/85
30/30 - 8s - loss: 0.1268 - lwlrap: 0.1155 - val_loss: 0.0784 - val_lwlrap: 0.8140
Epoch 10/85
30/30 - 8s - loss: 0.1092 - lwlrap: 0.1393 - val_loss: 0.0722 - val_lwlrap: 0.8424
Epoch 11/85
30/30 - 9s - loss: 0.0581 - lwlrap: 0.1342 - val_loss: 0.0539 - val_lwlrap: 0.8632
Epoch 12/85
30/30 - 8s - loss: 0.0612 - lwlrap: 0.1520 - val_loss: 0.0714 - val_lwlrap: 0.8399
Epoch 13/85

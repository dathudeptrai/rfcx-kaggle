
-------------   Fold 1 / 5  -------------

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-28/1/pretrained_best_fold0.h5

 -> Training Model 

Epoch 1/60
60/60 - 12s - loss: 0.2960 - lwlrap: 0.2064 - val_loss: 0.1781 - val_lwlrap: 0.2243
Epoch 2/60
60/60 - 11s - loss: 0.2334 - lwlrap: 0.3215 - val_loss: 0.1810 - val_lwlrap: 0.3007
Epoch 3/60
60/60 - 11s - loss: 0.1803 - lwlrap: 0.4540 - val_loss: 0.1625 - val_lwlrap: 0.4746
Epoch 4/60
60/60 - 11s - loss: 0.2630 - lwlrap: 0.5480 - val_loss: 0.1520 - val_lwlrap: 0.4953
Epoch 5/60
60/60 - 11s - loss: 0.1144 - lwlrap: 0.5588 - val_loss: 0.1696 - val_lwlrap: 0.5145
Epoch 6/60
60/60 - 11s - loss: 0.1690 - lwlrap: 0.6746 - val_loss: 0.1406 - val_lwlrap: 0.6206
Epoch 7/60
60/60 - 11s - loss: 0.1295 - lwlrap: 0.7594 - val_loss: 0.2672 - val_lwlrap: 0.6839
Epoch 8/60
60/60 - 11s - loss: 0.0551 - lwlrap: 0.8269 - val_loss: 0.1274 - val_lwlrap: 0.7424
Epoch 9/60
60/60 - 11s - loss: 0.1050 - lwlrap: 0.8887 - val_loss: 0.1058 - val_lwlrap: 0.7937
Epoch 10/60
60/60 - 11s - loss: 0.0332 - lwlrap: 0.9243 - val_loss: 0.1610 - val_lwlrap: 0.8090
Epoch 11/60
60/60 - 11s - loss: 0.0343 - lwlrap: 0.9461 - val_loss: 0.1127 - val_lwlrap: 0.8121
Epoch 12/60
60/60 - 11s - loss: 0.0311 - lwlrap: 0.9405 - val_loss: 0.1035 - val_lwlrap: 0.8113
Epoch 13/60
60/60 - 11s - loss: 0.0249 - lwlrap: 0.9506 - val_loss: 0.1456 - val_lwlrap: 0.8103
Epoch 14/60
60/60 - 11s - loss: 0.0379 - lwlrap: 0.9388 - val_loss: 0.1522 - val_lwlrap: 0.7901
Epoch 15/60
60/60 - 11s - loss: 0.0298 - lwlrap: 0.9343 - val_loss: 0.1003 - val_lwlrap: 0.8057
Epoch 16/60
60/60 - 11s - loss: 0.0351 - lwlrap: 0.9362 - val_loss: 0.1577 - val_lwlrap: 0.7591
Epoch 17/60
60/60 - 11s - loss: 0.0375 - lwlrap: 0.9650 - val_loss: 0.2000 - val_lwlrap: 0.8217
Epoch 18/60
60/60 - 11s - loss: 0.0123 - lwlrap: 0.9686 - val_loss: 0.2543 - val_lwlrap: 0.8367
Epoch 19/60
60/60 - 11s - loss: 0.0263 - lwlrap: 0.9821 - val_loss: 0.1159 - val_lwlrap: 0.8393
Epoch 20/60
60/60 - 11s - loss: 0.0223 - lwlrap: 0.9895 - val_loss: 0.1860 - val_lwlrap: 0.8261
Epoch 21/60
60/60 - 11s - loss: 0.0462 - lwlrap: 0.9929 - val_loss: 0.1206 - val_lwlrap: 0.8239
Epoch 22/60
60/60 - 11s - loss: 0.0117 - lwlrap: 0.9921 - val_loss: 0.1708 - val_lwlrap: 0.8280
Epoch 23/60
60/60 - 11s - loss: 0.0165 - lwlrap: 0.9921 - val_loss: 0.1238 - val_lwlrap: 0.8211
Epoch 24/60
60/60 - 11s - loss: 0.0041 - lwlrap: 0.9896 - val_loss: 0.1432 - val_lwlrap: 0.8413
Epoch 25/60
60/60 - 11s - loss: 0.0084 - lwlrap: 0.9828 - val_loss: 0.2100 - val_lwlrap: 0.8027
Epoch 26/60
60/60 - 11s - loss: 0.0083 - lwlrap: 0.9875 - val_loss: 0.2543 - val_lwlrap: 0.8296
Epoch 27/60
60/60 - 11s - loss: 0.0110 - lwlrap: 0.9874 - val_loss: 0.1688 - val_lwlrap: 0.8114
Epoch 28/60
60/60 - 11s - loss: 0.0102 - lwlrap: 0.9922 - val_loss: 0.2009 - val_lwlrap: 0.8309
Epoch 29/60
60/60 - 11s - loss: 0.0041 - lwlrap: 0.9928 - val_loss: 0.1939 - val_lwlrap: 0.8403
Epoch 30/60
60/60 - 11s - loss: 0.0023 - lwlrap: 0.9971 - val_loss: 0.1150 - val_lwlrap: 0.8381
Epoch 31/60
60/60 - 11s - loss: 0.0217 - lwlrap: 0.9971 - val_loss: 0.1335 - val_lwlrap: 0.8358
Epoch 32/60
60/60 - 11s - loss: 0.0017 - lwlrap: 0.9990 - val_loss: 0.2432 - val_lwlrap: 0.8479
Epoch 33/60
60/60 - 11s - loss: 0.0144 - lwlrap: 0.9976 - val_loss: 0.2679 - val_lwlrap: 0.8275
Epoch 34/60
60/60 - 11s - loss: 7.7681e-04 - lwlrap: 0.9964 - val_loss: 0.1432 - val_lwlrap: 0.8299
Epoch 35/60
60/60 - 11s - loss: 0.0110 - lwlrap: 0.9921 - val_loss: 0.1836 - val_lwlrap: 0.8219
Epoch 36/60
60/60 - 11s - loss: 0.0027 - lwlrap: 0.9967 - val_loss: 0.2357 - val_lwlrap: 0.8247
Epoch 37/60
60/60 - 11s - loss: 0.0038 - lwlrap: 0.9950 - val_loss: 0.2176 - val_lwlrap: 0.8214
Epoch 38/60
60/60 - 11s - loss: 0.0020 - lwlrap: 0.9973 - val_loss: 0.1706 - val_lwlrap: 0.8223
Epoch 39/60

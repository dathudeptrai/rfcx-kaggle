
-------------   Fold 1 / 5  -------------

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-28/1/pretrained_best_fold0.h5

 -> Training Model 

Epoch 1/60
60/60 - 11s - loss: 0.2733 - lwlrap: 0.3394 - val_loss: 0.1700 - val_lwlrap: 0.4978
Epoch 2/60
60/60 - 10s - loss: 0.1666 - lwlrap: 0.6852 - val_loss: 0.1755 - val_lwlrap: 0.5882
Epoch 3/60
60/60 - 10s - loss: 0.0808 - lwlrap: 0.7864 - val_loss: 0.1167 - val_lwlrap: 0.6532
Epoch 4/60
60/60 - 10s - loss: 0.1500 - lwlrap: 0.9024 - val_loss: 0.1795 - val_lwlrap: 0.7255
Epoch 5/60
60/60 - 10s - loss: 0.0414 - lwlrap: 0.8988 - val_loss: 0.2129 - val_lwlrap: 0.7456
Epoch 6/60
60/60 - 9s - loss: 0.1130 - lwlrap: 0.9146 - val_loss: 0.1562 - val_lwlrap: 0.6454
Epoch 7/60
60/60 - 9s - loss: 0.0555 - lwlrap: 0.9359 - val_loss: 0.1751 - val_lwlrap: 0.8078
Epoch 8/60
60/60 - 9s - loss: 0.0211 - lwlrap: 0.9688 - val_loss: 0.0798 - val_lwlrap: 0.7821
Epoch 9/60
60/60 - 10s - loss: 0.0300 - lwlrap: 0.9848 - val_loss: 0.1088 - val_lwlrap: 0.8362
Epoch 10/60
60/60 - 10s - loss: 0.0199 - lwlrap: 0.9884 - val_loss: 0.0805 - val_lwlrap: 0.8413
Epoch 11/60
60/60 - 9s - loss: 0.0075 - lwlrap: 0.9920 - val_loss: 0.1004 - val_lwlrap: 0.8238
Epoch 12/60
60/60 - 9s - loss: 0.0036 - lwlrap: 0.9904 - val_loss: 0.0934 - val_lwlrap: 0.8396
Epoch 13/60
60/60 - 9s - loss: 0.0035 - lwlrap: 0.9943 - val_loss: 0.0224 - val_lwlrap: 0.8303
Epoch 14/60
60/60 - 9s - loss: 0.0017 - lwlrap: 0.9930 - val_loss: 0.1762 - val_lwlrap: 0.8361
Epoch 15/60
60/60 - 9s - loss: 0.0071 - lwlrap: 0.9887 - val_loss: 0.2024 - val_lwlrap: 0.8107
Epoch 16/60
60/60 - 9s - loss: 0.0217 - lwlrap: 0.9926 - val_loss: 0.1649 - val_lwlrap: 0.8407
Epoch 17/60
60/60 - 9s - loss: 0.0279 - lwlrap: 0.9941 - val_loss: 0.2249 - val_lwlrap: 0.8142
Epoch 18/60
60/60 - 9s - loss: 0.0138 - lwlrap: 0.9893 - val_loss: 0.1010 - val_lwlrap: 0.8328
Epoch 19/60
60/60 - 9s - loss: 0.0098 - lwlrap: 0.9984 - val_loss: 0.1311 - val_lwlrap: 0.8289
Epoch 20/60
60/60 - 10s - loss: 0.0034 - lwlrap: 0.9982 - val_loss: 0.1293 - val_lwlrap: 0.8456
Epoch 21/60
60/60 - 9s - loss: 0.0031 - lwlrap: 0.9985 - val_loss: 0.1488 - val_lwlrap: 0.8404
Epoch 22/60
60/60 - 9s - loss: 0.0122 - lwlrap: 0.9981 - val_loss: 0.1770 - val_lwlrap: 0.8385
Epoch 23/60
60/60 - 9s - loss: 0.0029 - lwlrap: 0.9975 - val_loss: 0.0462 - val_lwlrap: 0.8351
Epoch 24/60
60/60 - 10s - loss: 0.0014 - lwlrap: 0.9981 - val_loss: 0.0619 - val_lwlrap: 0.8526
Epoch 25/60
60/60 - 9s - loss: 6.5724e-04 - lwlrap: 0.9989 - val_loss: 0.1701 - val_lwlrap: 0.8493
Epoch 26/60
60/60 - 9s - loss: 0.0035 - lwlrap: 0.9986 - val_loss: 0.1494 - val_lwlrap: 0.8216
Epoch 27/60
60/60 - 9s - loss: 0.0021 - lwlrap: 0.9984 - val_loss: 0.3477 - val_lwlrap: 0.8307
Epoch 28/60
60/60 - 9s - loss: 0.0037 - lwlrap: 0.9991 - val_loss: 0.2782 - val_lwlrap: 0.8194
Epoch 29/60
60/60 - 9s - loss: 0.0022 - lwlrap: 0.9982 - val_loss: 0.1804 - val_lwlrap: 0.8256
Epoch 30/60
60/60 - 9s - loss: 0.0021 - lwlrap: 0.9979 - val_loss: 0.0580 - val_lwlrap: 0.8378
Epoch 31/60
60/60 - 9s - loss: 0.0020 - lwlrap: 0.9990 - val_loss: 0.1028 - val_lwlrap: 0.8353
Epoch 32/60
60/60 - 9s - loss: 7.4435e-04 - lwlrap: 1.0000 - val_loss: 0.1701 - val_lwlrap: 0.8613
Epoch 33/60
60/60 - 9s - loss: 0.0030 - lwlrap: 0.9988 - val_loss: 0.2271 - val_lwlrap: 0.8543
Epoch 34/60
60/60 - 9s - loss: 0.0045 - lwlrap: 0.9987 - val_loss: 0.2989 - val_lwlrap: 0.8368
Epoch 35/60
60/60 - 9s - loss: 0.0022 - lwlrap: 0.9982 - val_loss: 0.3069 - val_lwlrap: 0.8358
Epoch 36/60
60/60 - 9s - loss: 0.0074 - lwlrap: 0.9974 - val_loss: 0.3820 - val_lwlrap: 0.8371
Epoch 37/60
60/60 - 9s - loss: 0.0039 - lwlrap: 0.9976 - val_loss: 0.3504 - val_lwlrap: 0.8316
Epoch 38/60
60/60 - 9s - loss: 8.5477e-04 - lwlrap: 0.9991 - val_loss: 0.3558 - val_lwlrap: 0.8507
Epoch 39/60
60/60 - 9s - loss: 6.9099e-04 - lwlrap: 0.9977 - val_loss: 0.3443 - val_lwlrap: 0.8420
Epoch 40/60
60/60 - 9s - loss: 0.0033 - lwlrap: 0.9998 - val_loss: 0.1721 - val_lwlrap: 0.8418
Epoch 41/60
60/60 - 9s - loss: 0.0191 - lwlrap: 0.9996 - val_loss: 0.1499 - val_lwlrap: 0.8455
Epoch 42/60
60/60 - 9s - loss: 0.0047 - lwlrap: 0.9998 - val_loss: 0.1622 - val_lwlrap: 0.8294
Epoch 43/60
60/60 - 9s - loss: 0.0011 - lwlrap: 0.9998 - val_loss: 0.2783 - val_lwlrap: 0.8344
Epoch 44/60
60/60 - 9s - loss: 4.9333e-04 - lwlrap: 1.0000 - val_loss: 0.1966 - val_lwlrap: 0.8410
Epoch 45/60
60/60 - 9s - loss: 3.6532e-04 - lwlrap: 0.9988 - val_loss: 0.3185 - val_lwlrap: 0.8323
Epoch 46/60
60/60 - 9s - loss: 6.4666e-04 - lwlrap: 0.9986 - val_loss: 0.2207 - val_lwlrap: 0.8253
Epoch 47/60
60/60 - 9s - loss: 3.6761e-04 - lwlrap: 0.9993 - val_loss: 0.1964 - val_lwlrap: 0.8463
Epoch 48/60
60/60 - 9s - loss: 0.0085 - lwlrap: 0.9996 - val_loss: 0.1648 - val_lwlrap: 0.8354
Epoch 49/60
60/60 - 9s - loss: 3.8039e-04 - lwlrap: 0.9997 - val_loss: 0.2311 - val_lwlrap: 0.8337
Epoch 50/60
60/60 - 9s - loss: 8.0956e-04 - lwlrap: 0.9996 - val_loss: 0.2888 - val_lwlrap: 0.8449
Epoch 51/60
60/60 - 9s - loss: 4.7013e-04 - lwlrap: 0.9995 - val_loss: 0.4435 - val_lwlrap: 0.8310
Epoch 52/60
60/60 - 9s - loss: 8.5759e-04 - lwlrap: 0.9988 - val_loss: 0.2598 - val_lwlrap: 0.8249

-------------   Fold 2 / 5  -------------

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-28/1/pretrained_best_fold1.h5

 -> Training Model 

Epoch 1/60


-------------   Fold 1 / 5  -------------

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-28/1/pretrained_best_fold0.h5

 -> Training Model 

Epoch 1/60
60/60 - 11s - loss: 0.2501 - lwlrap: 0.4403 - val_loss: 0.1731 - val_lwlrap: 0.5694
Epoch 2/60
60/60 - 9s - loss: 0.1769 - lwlrap: 0.7230 - val_loss: 0.1983 - val_lwlrap: 0.5722
Epoch 3/60
60/60 - 9s - loss: 0.0831 - lwlrap: 0.7899 - val_loss: 0.1315 - val_lwlrap: 0.6757
Epoch 4/60
60/60 - 9s - loss: 0.1685 - lwlrap: 0.8319 - val_loss: 0.1510 - val_lwlrap: 0.6849
Epoch 5/60
60/60 - 9s - loss: 0.0479 - lwlrap: 0.8238 - val_loss: 0.2232 - val_lwlrap: 0.7112
Epoch 6/60
60/60 - 9s - loss: 0.0763 - lwlrap: 0.8727 - val_loss: 0.0654 - val_lwlrap: 0.7905
Epoch 7/60
60/60 - 9s - loss: 0.0660 - lwlrap: 0.9184 - val_loss: 0.1823 - val_lwlrap: 0.8073
Epoch 8/60
60/60 - 9s - loss: 0.0152 - lwlrap: 0.9495 - val_loss: 0.0534 - val_lwlrap: 0.8079
Epoch 9/60
60/60 - 9s - loss: 0.0182 - lwlrap: 0.9738 - val_loss: 0.0918 - val_lwlrap: 0.8501
Epoch 10/60
60/60 - 9s - loss: 0.0204 - lwlrap: 0.9834 - val_loss: 0.0303 - val_lwlrap: 0.8442
Epoch 11/60
60/60 - 9s - loss: 0.0056 - lwlrap: 0.9912 - val_loss: 0.0493 - val_lwlrap: 0.8614
Epoch 12/60
60/60 - 9s - loss: 0.0099 - lwlrap: 0.9881 - val_loss: 0.0262 - val_lwlrap: 0.8519
Epoch 13/60
60/60 - 9s - loss: 0.0041 - lwlrap: 0.9905 - val_loss: 0.1243 - val_lwlrap: 0.8502
Epoch 14/60
60/60 - 9s - loss: 0.0062 - lwlrap: 0.9871 - val_loss: 0.0310 - val_lwlrap: 0.8502
Epoch 15/60
60/60 - 9s - loss: 0.0205 - lwlrap: 0.9765 - val_loss: 0.1087 - val_lwlrap: 0.8212
Epoch 16/60
60/60 - 9s - loss: 0.0085 - lwlrap: 0.9774 - val_loss: 0.3833 - val_lwlrap: 0.8301
Epoch 17/60
60/60 - 9s - loss: 0.0126 - lwlrap: 0.9830 - val_loss: 0.4671 - val_lwlrap: 0.8355
Epoch 18/60
60/60 - 9s - loss: 0.0060 - lwlrap: 0.9892 - val_loss: 0.1455 - val_lwlrap: 0.8484
Epoch 19/60
60/60 - 9s - loss: 0.0073 - lwlrap: 0.9954 - val_loss: 0.0937 - val_lwlrap: 0.8550
Epoch 20/60
60/60 - 9s - loss: 0.0139 - lwlrap: 0.9963 - val_loss: 0.1395 - val_lwlrap: 0.8702
Epoch 21/60
60/60 - 9s - loss: 0.0226 - lwlrap: 0.9991 - val_loss: 0.1428 - val_lwlrap: 0.8507
Epoch 22/60
60/60 - 9s - loss: 0.0174 - lwlrap: 0.9976 - val_loss: 0.1890 - val_lwlrap: 0.8647
Epoch 23/60
60/60 - 9s - loss: 0.0097 - lwlrap: 0.9974 - val_loss: 0.0145 - val_lwlrap: 0.8599
Epoch 24/60
60/60 - 9s - loss: 6.9020e-04 - lwlrap: 0.9976 - val_loss: 0.1236 - val_lwlrap: 0.8716
Epoch 25/60
60/60 - 9s - loss: 7.6164e-04 - lwlrap: 0.9988 - val_loss: 0.1746 - val_lwlrap: 0.8538
Epoch 26/60
60/60 - 9s - loss: 0.0041 - lwlrap: 0.9971 - val_loss: 0.1380 - val_lwlrap: 0.8359
Epoch 27/60
60/60 - 9s - loss: 0.0011 - lwlrap: 0.9965 - val_loss: 0.2603 - val_lwlrap: 0.8182
Epoch 28/60
60/60 - 9s - loss: 0.0023 - lwlrap: 0.9987 - val_loss: 0.2336 - val_lwlrap: 0.8429
Epoch 29/60
60/60 - 9s - loss: 0.0067 - lwlrap: 0.9988 - val_loss: 0.1914 - val_lwlrap: 0.8506
Epoch 30/60
60/60 - 9s - loss: 6.2293e-04 - lwlrap: 0.9982 - val_loss: 0.0574 - val_lwlrap: 0.8670
Epoch 31/60
60/60 - 9s - loss: 0.0069 - lwlrap: 0.9983 - val_loss: 0.0858 - val_lwlrap: 0.8493
Epoch 32/60
60/60 - 9s - loss: 8.5687e-04 - lwlrap: 0.9992 - val_loss: 0.0610 - val_lwlrap: 0.8660
Epoch 33/60
60/60 - 9s - loss: 0.0182 - lwlrap: 0.9991 - val_loss: 0.2585 - val_lwlrap: 0.8524
Epoch 34/60
60/60 - 9s - loss: 4.3948e-04 - lwlrap: 0.9987 - val_loss: 0.2731 - val_lwlrap: 0.8433
Epoch 35/60
60/60 - 9s - loss: 0.0013 - lwlrap: 0.9981 - val_loss: 0.2437 - val_lwlrap: 0.8450
Epoch 36/60
60/60 - 9s - loss: 9.6994e-04 - lwlrap: 0.9982 - val_loss: 0.4285 - val_lwlrap: 0.8514
Epoch 37/60
60/60 - 9s - loss: 7.3995e-04 - lwlrap: 0.9967 - val_loss: 0.3594 - val_lwlrap: 0.8501
Epoch 38/60
60/60 - 9s - loss: 9.0000e-04 - lwlrap: 0.9988 - val_loss: 0.3656 - val_lwlrap: 0.8469
Epoch 39/60
60/60 - 9s - loss: 0.0012 - lwlrap: 0.9980 - val_loss: 0.2896 - val_lwlrap: 0.8525
Epoch 40/60
60/60 - 9s - loss: 0.0012 - lwlrap: 0.9991 - val_loss: 0.1628 - val_lwlrap: 0.8374
Epoch 41/60
60/60 - 9s - loss: 0.0211 - lwlrap: 0.9995 - val_loss: 0.1432 - val_lwlrap: 0.8516
Epoch 42/60
60/60 - 9s - loss: 0.0011 - lwlrap: 0.9998 - val_loss: 0.1131 - val_lwlrap: 0.8397
Epoch 43/60
60/60 - 9s - loss: 0.0013 - lwlrap: 0.9989 - val_loss: 0.2106 - val_lwlrap: 0.8519
Epoch 44/60
60/60 - 9s - loss: 5.1179e-04 - lwlrap: 1.0000 - val_loss: 0.2778 - val_lwlrap: 0.8518

-------------   Fold 2 / 5  -------------

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-28/1/pretrained_best_fold1.h5

 -> Training Model 

Epoch 1/60
60/60 - 11s - loss: 0.1295 - lwlrap: 0.5293 - val_loss: 0.1117 - val_lwlrap: 0.7620
Epoch 2/60
60/60 - 9s - loss: 0.2293 - lwlrap: 0.8724 - val_loss: 0.0883 - val_lwlrap: 0.7220
Epoch 3/60
60/60 - 9s - loss: 0.1574 - lwlrap: 0.8573 - val_loss: 0.1864 - val_lwlrap: 0.5458
Epoch 4/60
60/60 - 9s - loss: 0.0647 - lwlrap: 0.8452 - val_loss: 0.1317 - val_lwlrap: 0.5738
Epoch 5/60
60/60 - 9s - loss: 0.0962 - lwlrap: 0.8599 - val_loss: 0.1140 - val_lwlrap: 0.7005
Epoch 6/60
60/60 - 10s - loss: 0.0681 - lwlrap: 0.9055 - val_loss: 0.0455 - val_lwlrap: 0.8088
Epoch 7/60
60/60 - 10s - loss: 0.0522 - lwlrap: 0.9220 - val_loss: 0.0536 - val_lwlrap: 0.8441
Epoch 8/60
60/60 - 9s - loss: 0.0222 - lwlrap: 0.9619 - val_loss: 0.0606 - val_lwlrap: 0.8233
Epoch 9/60
60/60 - 9s - loss: 0.0117 - lwlrap: 0.9791 - val_loss: 0.0366 - val_lwlrap: 0.8563
Epoch 10/60
60/60 - 10s - loss: 0.0070 - lwlrap: 0.9822 - val_loss: 0.0472 - val_lwlrap: 0.8774
Epoch 11/60
60/60 - 10s - loss: 0.0068 - lwlrap: 0.9925 - val_loss: 0.0420 - val_lwlrap: 0.8906
Epoch 12/60
60/60 - 9s - loss: 0.0153 - lwlrap: 0.9896 - val_loss: 0.0446 - val_lwlrap: 0.8823
Epoch 13/60
60/60 - 9s - loss: 0.0129 - lwlrap: 0.9925 - val_loss: 0.0272 - val_lwlrap: 0.8749
Epoch 14/60
60/60 - 9s - loss: 0.0166 - lwlrap: 0.9888 - val_loss: 0.0516 - val_lwlrap: 0.8675
Epoch 15/60
60/60 - 9s - loss: 0.0084 - lwlrap: 0.9909 - val_loss: 0.0371 - val_lwlrap: 0.8757
Epoch 16/60
60/60 - 9s - loss: 0.0087 - lwlrap: 0.9835 - val_loss: 0.0719 - val_lwlrap: 0.8597
Epoch 17/60
60/60 - 9s - loss: 0.0050 - lwlrap: 0.9803 - val_loss: 0.1012 - val_lwlrap: 0.8671
Epoch 18/60
60/60 - 9s - loss: 0.0253 - lwlrap: 0.9940 - val_loss: 0.1265 - val_lwlrap: 0.8685
Epoch 19/60
60/60 - 9s - loss: 0.0197 - lwlrap: 0.9961 - val_loss: 0.0564 - val_lwlrap: 0.8805
Epoch 20/60
60/60 - 9s - loss: 0.0055 - lwlrap: 0.9973 - val_loss: 0.0841 - val_lwlrap: 0.8710
Epoch 21/60
60/60 - 9s - loss: 0.0015 - lwlrap: 0.9981 - val_loss: 0.0594 - val_lwlrap: 0.8825
Epoch 22/60
60/60 - 9s - loss: 0.0017 - lwlrap: 0.9986 - val_loss: 0.0936 - val_lwlrap: 0.8796
Epoch 23/60
60/60 - 9s - loss: 0.0014 - lwlrap: 0.9990 - val_loss: 0.0571 - val_lwlrap: 0.8789
Epoch 24/60
60/60 - 9s - loss: 0.0105 - lwlrap: 0.9981 - val_loss: 0.0692 - val_lwlrap: 0.8901
Epoch 25/60
60/60 - 9s - loss: 0.0015 - lwlrap: 0.9979 - val_loss: 0.1100 - val_lwlrap: 0.8772
Epoch 26/60
60/60 - 9s - loss: 0.0020 - lwlrap: 0.9977 - val_loss: 0.0792 - val_lwlrap: 0.8583
Epoch 27/60
60/60 - 9s - loss: 0.0111 - lwlrap: 0.9997 - val_loss: 0.0647 - val_lwlrap: 0.8896
Epoch 28/60
60/60 - 9s - loss: 0.0148 - lwlrap: 0.9973 - val_loss: 0.1010 - val_lwlrap: 0.8745
Epoch 29/60
60/60 - 10s - loss: 0.0017 - lwlrap: 0.9980 - val_loss: 0.0813 - val_lwlrap: 0.8911
Epoch 30/60
60/60 - 9s - loss: 6.1773e-04 - lwlrap: 0.9998 - val_loss: 0.0878 - val_lwlrap: 0.8838
Epoch 31/60
60/60 - 9s - loss: 0.0054 - lwlrap: 0.9979 - val_loss: 0.0713 - val_lwlrap: 0.8630
Epoch 32/60
60/60 - 9s - loss: 7.5489e-04 - lwlrap: 0.9992 - val_loss: 0.0507 - val_lwlrap: 0.8668
Epoch 33/60
60/60 - 9s - loss: 6.8689e-04 - lwlrap: 0.9991 - val_loss: 0.0551 - val_lwlrap: 0.8894
Epoch 34/60

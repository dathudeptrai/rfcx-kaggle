
-------------   Fold 1 / 5  -------------

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-31/8/pretrained_best_fold0.h5

 -> Training Model 

Epoch 1/60
30/30 - 10s - loss: 0.0730 - lwlrap: 0.5469 - val_loss: 0.0821 - val_lwlrap: 0.7128
Epoch 2/60
30/30 - 8s - loss: 0.0667 - lwlrap: 0.7439 - val_loss: 0.0697 - val_lwlrap: 0.6979
Epoch 3/60
30/30 - 8s - loss: 0.0538 - lwlrap: 0.7060 - val_loss: 0.0716 - val_lwlrap: 0.7001
Epoch 4/60
30/30 - 8s - loss: 0.0436 - lwlrap: 0.7766 - val_loss: 0.0715 - val_lwlrap: 0.7104
Epoch 5/60
30/30 - 8s - loss: 0.0334 - lwlrap: 0.8058 - val_loss: 0.0527 - val_lwlrap: 0.7283
Epoch 6/60
30/30 - 8s - loss: 0.0274 - lwlrap: 0.8369 - val_loss: 0.0524 - val_lwlrap: 0.7538
Epoch 7/60
30/30 - 8s - loss: 0.0229 - lwlrap: 0.8557 - val_loss: 0.0587 - val_lwlrap: 0.7711
Epoch 8/60
30/30 - 8s - loss: 0.0198 - lwlrap: 0.8534 - val_loss: 0.0476 - val_lwlrap: 0.7886
Epoch 9/60
30/30 - 8s - loss: 0.0396 - lwlrap: 0.8463 - val_loss: 0.0525 - val_lwlrap: 0.7436
Epoch 10/60
30/30 - 8s - loss: 0.0346 - lwlrap: 0.7870 - val_loss: 0.0443 - val_lwlrap: 0.7634
Epoch 11/60
30/30 - 8s - loss: 0.0535 - lwlrap: 0.8739 - val_loss: 0.0415 - val_lwlrap: 0.7504
Epoch 12/60
30/30 - 8s - loss: 0.0267 - lwlrap: 0.8697 - val_loss: 0.0499 - val_lwlrap: 0.8033
Epoch 13/60
30/30 - 8s - loss: 0.0184 - lwlrap: 0.9220 - val_loss: 0.0478 - val_lwlrap: 0.8116
Epoch 14/60
30/30 - 8s - loss: 0.0147 - lwlrap: 0.9479 - val_loss: 0.0398 - val_lwlrap: 0.7789
Epoch 15/60
30/30 - 8s - loss: 0.0113 - lwlrap: 0.9472 - val_loss: 0.0495 - val_lwlrap: 0.7999
Epoch 16/60
30/30 - 8s - loss: 0.0132 - lwlrap: 0.9620 - val_loss: 0.0471 - val_lwlrap: 0.8061
Epoch 17/60
30/30 - 8s - loss: 0.0135 - lwlrap: 0.9734 - val_loss: 0.0522 - val_lwlrap: 0.8132
Epoch 18/60
30/30 - 8s - loss: 0.0087 - lwlrap: 0.9784 - val_loss: 0.0470 - val_lwlrap: 0.8003
Epoch 19/60
30/30 - 8s - loss: 0.0056 - lwlrap: 0.9868 - val_loss: 0.0566 - val_lwlrap: 0.8094
Epoch 20/60
30/30 - 8s - loss: 0.0072 - lwlrap: 0.9837 - val_loss: 0.0555 - val_lwlrap: 0.8193
Epoch 21/60
30/30 - 8s - loss: 0.0050 - lwlrap: 0.9884 - val_loss: 0.0512 - val_lwlrap: 0.8283
Epoch 22/60
30/30 - 8s - loss: 0.0050 - lwlrap: 0.9898 - val_loss: 0.0631 - val_lwlrap: 0.8145
Epoch 23/60
30/30 - 8s - loss: 0.0065 - lwlrap: 0.9911 - val_loss: 0.0570 - val_lwlrap: 0.8246
Epoch 24/60
30/30 - 8s - loss: 0.0037 - lwlrap: 0.9932 - val_loss: 0.0598 - val_lwlrap: 0.8011
Epoch 25/60
30/30 - 8s - loss: 0.0088 - lwlrap: 0.9908 - val_loss: 0.0484 - val_lwlrap: 0.7982
Epoch 26/60
30/30 - 8s - loss: 0.0053 - lwlrap: 0.9931 - val_loss: 0.0508 - val_lwlrap: 0.8239
Epoch 27/60
30/30 - 8s - loss: 0.0088 - lwlrap: 0.9935 - val_loss: 0.0470 - val_lwlrap: 0.8229
Epoch 28/60
30/30 - 8s - loss: 0.0047 - lwlrap: 0.9903 - val_loss: 0.0578 - val_lwlrap: 0.8257
Epoch 29/60
30/30 - 8s - loss: 0.0115 - lwlrap: 0.9925 - val_loss: 0.0493 - val_lwlrap: 0.8249
Epoch 30/60
30/30 - 8s - loss: 0.0049 - lwlrap: 0.9818 - val_loss: 0.0525 - val_lwlrap: 0.8227
Epoch 31/60
30/30 - 8s - loss: 0.0080 - lwlrap: 0.9889 - val_loss: 0.0516 - val_lwlrap: 0.7980
Epoch 32/60
30/30 - 8s - loss: 0.0075 - lwlrap: 0.9896 - val_loss: 0.0544 - val_lwlrap: 0.8108
Epoch 33/60
30/30 - 8s - loss: 0.0053 - lwlrap: 0.9878 - val_loss: 0.0552 - val_lwlrap: 0.8351
Epoch 34/60
30/30 - 8s - loss: 0.0093 - lwlrap: 0.9918 - val_loss: 0.0453 - val_lwlrap: 0.8051
Epoch 35/60
30/30 - 8s - loss: 0.0052 - lwlrap: 0.9924 - val_loss: 0.0597 - val_lwlrap: 0.8394
Epoch 36/60
30/30 - 8s - loss: 0.0055 - lwlrap: 0.9947 - val_loss: 0.0502 - val_lwlrap: 0.8306
Epoch 37/60
30/30 - 8s - loss: 0.0026 - lwlrap: 0.9986 - val_loss: 0.0510 - val_lwlrap: 0.8258
Epoch 38/60
30/30 - 8s - loss: 0.0043 - lwlrap: 0.9971 - val_loss: 0.0588 - val_lwlrap: 0.8259
Epoch 39/60
30/30 - 8s - loss: 0.0034 - lwlrap: 0.9990 - val_loss: 0.0465 - val_lwlrap: 0.8221
Epoch 40/60
30/30 - 8s - loss: 0.0043 - lwlrap: 0.9976 - val_loss: 0.0538 - val_lwlrap: 0.8262
Epoch 41/60
30/30 - 8s - loss: 0.0026 - lwlrap: 0.9990 - val_loss: 0.0527 - val_lwlrap: 0.8080
Epoch 42/60
30/30 - 8s - loss: 0.0022 - lwlrap: 0.9995 - val_loss: 0.0551 - val_lwlrap: 0.7935
Epoch 43/60
30/30 - 8s - loss: 0.0035 - lwlrap: 0.9996 - val_loss: 0.0628 - val_lwlrap: 0.8288
Epoch 44/60
30/30 - 8s - loss: 0.0069 - lwlrap: 0.9992 - val_loss: 0.0603 - val_lwlrap: 0.8211
Epoch 45/60
30/30 - 8s - loss: 0.0036 - lwlrap: 0.9996 - val_loss: 0.0696 - val_lwlrap: 0.7974
Epoch 46/60
30/30 - 8s - loss: 0.0033 - lwlrap: 0.9991 - val_loss: 0.0696 - val_lwlrap: 0.8005
Epoch 47/60
30/30 - 8s - loss: 0.0044 - lwlrap: 0.9988 - val_loss: 0.0700 - val_lwlrap: 0.8187
Epoch 48/60
30/30 - 8s - loss: 0.0028 - lwlrap: 0.9986 - val_loss: 0.0594 - val_lwlrap: 0.7972
Epoch 49/60
30/30 - 8s - loss: 0.0077 - lwlrap: 0.9992 - val_loss: 0.0729 - val_lwlrap: 0.8017
Epoch 50/60
30/30 - 8s - loss: 0.0025 - lwlrap: 0.9973 - val_loss: 0.0674 - val_lwlrap: 0.8057
Epoch 51/60
30/30 - 8s - loss: 0.0015 - lwlrap: 1.0000 - val_loss: 0.0604 - val_lwlrap: 0.7813
Epoch 52/60
30/30 - 8s - loss: 0.0063 - lwlrap: 0.9984 - val_loss: 0.0641 - val_lwlrap: 0.7884
Epoch 53/60
30/30 - 8s - loss: 0.0035 - lwlrap: 0.9995 - val_loss: 0.0594 - val_lwlrap: 0.8189
Epoch 54/60
30/30 - 8s - loss: 0.0036 - lwlrap: 0.9986 - val_loss: 0.0549 - val_lwlrap: 0.7889
Epoch 55/60
30/30 - 8s - loss: 0.0028 - lwlrap: 0.9994 - val_loss: 0.0545 - val_lwlrap: 0.8248

-------------   Fold 2 / 5  -------------

 -> Preparing Data 

 -> Preparing Model 

 -> Loading weights from ../logs/2021-01-31/8/pretrained_best_fold1.h5

 -> Training Model 

Epoch 1/60
30/30 - 10s - loss: 0.0424 - lwlrap: 0.4926 - val_loss: 0.0576 - val_lwlrap: 0.7078
Epoch 2/60
30/30 - 8s - loss: 0.0332 - lwlrap: 0.7924 - val_loss: 0.0551 - val_lwlrap: 0.7878
Epoch 3/60
30/30 - 8s - loss: 0.0197 - lwlrap: 0.8702 - val_loss: 0.0475 - val_lwlrap: 0.8283
Epoch 4/60
30/30 - 8s - loss: 0.0155 - lwlrap: 0.8814 - val_loss: 0.0399 - val_lwlrap: 0.7949
Epoch 5/60
30/30 - 8s - loss: 0.0223 - lwlrap: 0.8994 - val_loss: 0.0384 - val_lwlrap: 0.8051
Epoch 6/60
30/30 - 8s - loss: 0.0172 - lwlrap: 0.9001 - val_loss: 0.0376 - val_lwlrap: 0.8122
Epoch 7/60
30/30 - 8s - loss: 0.0396 - lwlrap: 0.8576 - val_loss: 0.0353 - val_lwlrap: 0.7706
Epoch 8/60
30/30 - 8s - loss: 0.0275 - lwlrap: 0.8860 - val_loss: 0.0327 - val_lwlrap: 0.8201
Epoch 9/60
30/30 - 8s - loss: 0.0166 - lwlrap: 0.8981 - val_loss: 0.0309 - val_lwlrap: 0.7300
Epoch 10/60
30/30 - 8s - loss: 0.0237 - lwlrap: 0.9045 - val_loss: 0.0327 - val_lwlrap: 0.7567
Epoch 11/60
30/30 - 8s - loss: 0.0164 - lwlrap: 0.9324 - val_loss: 0.0287 - val_lwlrap: 0.8018
Epoch 12/60
30/30 - 8s - loss: 0.0160 - lwlrap: 0.9475 - val_loss: 0.0289 - val_lwlrap: 0.8567
Epoch 13/60
30/30 - 8s - loss: 0.0148 - lwlrap: 0.9649 - val_loss: 0.0251 - val_lwlrap: 0.8397
Epoch 14/60
30/30 - 8s - loss: 0.0063 - lwlrap: 0.9822 - val_loss: 0.0304 - val_lwlrap: 0.8484
Epoch 15/60
30/30 - 8s - loss: 0.0191 - lwlrap: 0.9804 - val_loss: 0.0320 - val_lwlrap: 0.8644
Epoch 16/60
30/30 - 8s - loss: 0.0049 - lwlrap: 0.9818 - val_loss: 0.0309 - val_lwlrap: 0.8379
Epoch 17/60
30/30 - 8s - loss: 0.0056 - lwlrap: 0.9935 - val_loss: 0.0285 - val_lwlrap: 0.8576
Epoch 18/60
30/30 - 8s - loss: 0.0165 - lwlrap: 0.9931 - val_loss: 0.0300 - val_lwlrap: 0.8520
Epoch 19/60
30/30 - 8s - loss: 0.0080 - lwlrap: 0.9886 - val_loss: 0.0297 - val_lwlrap: 0.8794
Epoch 20/60
30/30 - 8s - loss: 0.0030 - lwlrap: 0.9932 - val_loss: 0.0311 - val_lwlrap: 0.8759
Epoch 21/60
30/30 - 8s - loss: 0.0084 - lwlrap: 0.9957 - val_loss: 0.0327 - val_lwlrap: 0.8682
Epoch 22/60
30/30 - 8s - loss: 0.0062 - lwlrap: 0.9964 - val_loss: 0.0324 - val_lwlrap: 0.8579
Epoch 23/60
30/30 - 8s - loss: 0.0058 - lwlrap: 0.9942 - val_loss: 0.0363 - val_lwlrap: 0.8643
Epoch 24/60
30/30 - 8s - loss: 0.0050 - lwlrap: 0.9969 - val_loss: 0.0350 - val_lwlrap: 0.8717
Epoch 25/60
30/30 - 8s - loss: 0.0050 - lwlrap: 0.9964 - val_loss: 0.0334 - val_lwlrap: 0.8584
Epoch 26/60
30/30 - 8s - loss: 0.0060 - lwlrap: 0.9927 - val_loss: 0.0359 - val_lwlrap: 0.8668
Epoch 27/60
30/30 - 8s - loss: 0.0057 - lwlrap: 0.9946 - val_loss: 0.0345 - val_lwlrap: 0.8792
Epoch 28/60
30/30 - 8s - loss: 0.0073 - lwlrap: 0.9924 - val_loss: 0.0389 - val_lwlrap: 0.8506
Epoch 29/60
30/30 - 8s - loss: 0.0067 - lwlrap: 0.9967 - val_loss: 0.0407 - val_lwlrap: 0.8339
Epoch 30/60
30/30 - 8s - loss: 0.0069 - lwlrap: 0.9944 - val_loss: 0.0418 - val_lwlrap: 0.8241
Epoch 31/60
30/30 - 8s - loss: 0.0035 - lwlrap: 0.9977 - val_loss: 0.0440 - val_lwlrap: 0.8553
Epoch 32/60
30/30 - 8s - loss: 0.0076 - lwlrap: 0.9949 - val_loss: 0.0486 - val_lwlrap: 0.8265
Epoch 33/60
[autoreload of backbones failed: Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py", line 245, in check
    superreload(m, reload, self.old_objects)
  File "/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py", line 394, in superreload
    module = reload(module)
  File "/usr/lib/python3.6/imp.py", line 315, in reload
    return importlib.reload(module)
  File "/usr/lib/python3.6/importlib/__init__.py", line 166, in reload
    _bootstrap._exec(spec, module)
  File "<frozen importlib._bootstrap>", line 618, in _exec
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/tf/notebooks/src/backbones/__init__.py", line 12, in <module>
    from backbones.resnext import ResNeXt50
  File "/tf/notebooks/src/backbones/resnext.py", line 2, in <module>
    from tf2_resnets.resnet import ResNet
  File "/usr/local/lib/python3.6/dist-packages/tf2_resnets/resnet.py", line 298, in <module>
    ret=imagenet_utils.PREPROCESS_INPUT_RET_DOC_TORCH)
KeyError: 'error'
]
The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload
/tf/notebooks/src
Traceback (most recent call last):


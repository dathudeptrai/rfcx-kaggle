{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/notebooks/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas\n",
    "except:\n",
    "    !pip install pandas iterative-stratification nlpaug==0.0.20 tqdm click tensorflow_probability==0.11.1 tf2_resnets tensorflow_addons==0.11.1 image-classifiers==0.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import click\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "from train import get_model\n",
    "from params import TEST_MELS_PATH, TRAIN_MELS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "for i in range(len(physical_devices)):\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[i], True)\n",
    "\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_s_e_window_sliding(sample_len, win_size, step_size):\n",
    "    start = 0\n",
    "    end = win_size\n",
    "    s_e = []\n",
    "    s_e.append([start, end])\n",
    "    while end < sample_len:\n",
    "        start += step_size\n",
    "        end = start + win_size\n",
    "        s_e.append([start, end])\n",
    "\n",
    "    s_e[-1][0] -= s_e[-1][1] - sample_len\n",
    "    s_e[-1][1] = sample_len\n",
    "    return s_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(model_name, df, checkpoints_path, fold, root=TEST_MELS_PATH):\n",
    "    all_checkpoints = sorted(\n",
    "        glob.glob(os.path.join(checkpoints_path, f\"fold{fold}\", \"model-*.h5\"))\n",
    "    )\n",
    "\n",
    "    model = get_model(\n",
    "        model_name=model_name,\n",
    "        saved_path=checkpoints_path,\n",
    "        pretrained_with_contrastive=False,\n",
    "        pretrained_path=all_checkpoints[-1],\n",
    "    )\n",
    "    \n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def predict(data):\n",
    "        out = model(data, training=False)\n",
    "        return out\n",
    "\n",
    "    lwlrap_at_scale = np.load(\n",
    "        os.path.join(CP_FOLDER, f\"lwlrap_at_scale_{fold}.npy\")\n",
    "    )  # [24, n_scales]\n",
    "\n",
    "    scales_to_compute = []\n",
    "    for c in range(NUM_CLASSES):\n",
    "        best_score = lwlrap_at_scale[:, c].max()\n",
    "        best_indexes = [i for i, j in enumerate(lwlrap_at_scale[:, c]) if j == best_score]\n",
    "        scales_to_compute.append(SCALES[best_indexes[-1]])\n",
    "    scales_to_compute = sorted(np.unique(scales_to_compute))\n",
    "    \n",
    "#     scales_to_compute = [SCALES[i] for i in sorted(np.unique(lwlrap_at_scale.argmax(0)))]\n",
    "    preds = {}\n",
    "\n",
    "    for scale in scales_to_compute:\n",
    "        print(\" -> Predicting at scale\", scale)\n",
    "        preds_scale = []\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            recording_id = df.iloc[i][\"recording_id\"]\n",
    "            mel = np.load(os.path.join(root, recording_id + \".npy\"))\n",
    "            \n",
    "            crops = generate_s_e_window_sliding(len(mel), win_size=scale, step_size=scale)\n",
    "                \n",
    "            mel_chunks = [mel[s:e] for (s, e) in crops]\n",
    "            mel_chunks = np.array(mel_chunks).reshape((-1, scale, NUM_FEATURES, 3))\n",
    "            \n",
    "            clip_preds = predict(mel_chunks)  # n_chunks x 24\n",
    "            \n",
    "            preds_scale.append(tf.nn.sigmoid(clip_preds).numpy())\n",
    "        preds[scale] = np.array(preds_scale)  # n_samples x n_chunks x 24\n",
    "        \n",
    "        if preds[scale].max() < 0.9:\n",
    "            print('Warning, low max value.')\n",
    "\n",
    "    print('\\n -> Aggregating predictions')\n",
    "    \n",
    "    pred_agg = np.zeros((len(df), NUM_CLASSES))\n",
    "    preds_multiscale = []\n",
    "    for c in range(NUM_CLASSES):\n",
    "        \n",
    "#         scale_to_use = SCALES[lwlrap_at_scale[:, c].argmax()]\n",
    "        best_score = lwlrap_at_scale[:, c].max()\n",
    "        best_indexes = [i for i, j in enumerate(lwlrap_at_scale[:, c]) if j == best_score]\n",
    "        scale_to_use = SCALES[best_indexes[-1]]\n",
    "        \n",
    "        pred_class = preds[scale_to_use][:, :, c]\n",
    "        preds_multiscale.append(pred_class)\n",
    "        pred_agg[:, c] = pred_class.max(-1)\n",
    "\n",
    "    return pred_agg, preds_multiscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preds_to_pl(df, cp_folder, root=\"preds\"):\n",
    "    crops = []\n",
    "    for scale in SCALES:\n",
    "        crops.append(generate_s_e_window_sliding(SPEC_LENGTH, win_size=scale, step_size=scale))\n",
    "        \n",
    "    all_preds = []\n",
    "    for fold_idx in tqdm(range(5)):\n",
    "        preds = np.zeros((len(df), SPEC_LENGTH, NUM_CLASSES))\n",
    "        \n",
    "        for c in range(NUM_CLASSES):\n",
    "            pred = np.load(cp_folder + f\"{root}/pred_multiscale_{fold_idx}_{c}.npy\")\n",
    "            \n",
    "            associated_crops = [c for c in crops if len(c) == len(pred[0])][0]\n",
    "\n",
    "            for i in range(len(df)):\n",
    "                for crop in range(len(associated_crops)):\n",
    "                    start, end = associated_crops[crop][0], associated_crops[crop][1]\n",
    "                    preds[i, start:end, c] = pred[i][crop]\n",
    "\n",
    "        all_preds.append(preds)\n",
    "        \n",
    "    return np.array(all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP_FOLDER = LOG_PATH + \"2021-02-04/9/\"\n",
    "CP_FOLDER = LOG_PATH + \"2021-02-04/12/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "# #    'densenet121',\n",
    "    'resnet18',\n",
    "    'resnet34',\n",
    "    'resnext50',\n",
    "    'efficientnetb2',\n",
    "    \"xception\",\n",
    "    'resnest50',\n",
    "#     'efficientnetb3',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP_FOLDERS = [\n",
    "#     LOG_PATH + \"2021-02-04/9/\"\n",
    "    \"../logs/2021-02-14/8/\",\n",
    "    \"../logs/2021-02-14/9/\",\n",
    "    \"../logs/2021-02-14/10/\",\n",
    "    \"../logs/2021-02-15/0/\",\n",
    "    \"../logs/2021-02-15/1/\",\n",
    "    \"../logs/2021-02-15/2/\",\n",
    "#     \"../logs/2021-02-15/3/\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(DATA_PATH + \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################\n",
      " -> Log folder : ../logs/2021-02-14/8/\n",
      "########################\n",
      "\n",
      "\n",
      "-------------   Fold 1 / 5  -------------\n",
      "\n",
      " -> Loading weights from ../logs/2021-02-14/8/fold0/model-0.831-0.085.h5\n",
      "\n",
      " -> Predicting at scale 32\n",
      " -> Predicting at scale 64\n",
      " -> Predicting at scale 128\n",
      " -> Predicting at scale 192\n",
      " -> Predicting at scale 256\n",
      " -> Predicting at scale 384\n",
      " -> Predicting at scale 448\n",
      " -> Predicting at scale 512\n",
      "\n",
      " -> Aggregating predictions\n",
      "\n",
      "-------------   Fold 2 / 5  -------------\n",
      "\n",
      " -> Loading weights from ../logs/2021-02-14/8/fold1/model-0.914-0.081.h5\n",
      "\n",
      " -> Predicting at scale 32\n",
      " -> Predicting at scale 64\n",
      " -> Predicting at scale 128\n",
      " -> Predicting at scale 192\n",
      " -> Predicting at scale 256\n",
      " -> Predicting at scale 384\n",
      " -> Predicting at scale 448\n",
      " -> Predicting at scale 512\n",
      "\n",
      " -> Aggregating predictions\n",
      "\n",
      "-------------   Fold 3 / 5  -------------\n",
      "\n",
      " -> Loading weights from ../logs/2021-02-14/8/fold2/model-0.863-0.067.h5\n",
      "\n",
      " -> Predicting at scale 32\n",
      " -> Predicting at scale 64\n",
      " -> Predicting at scale 128\n",
      " -> Predicting at scale 192\n",
      " -> Predicting at scale 256\n",
      " -> Predicting at scale 320\n",
      " -> Predicting at scale 384\n",
      " -> Predicting at scale 512\n",
      "\n",
      " -> Aggregating predictions\n",
      "\n",
      "-------------   Fold 4 / 5  -------------\n",
      "\n",
      " -> Loading weights from ../logs/2021-02-14/8/fold3/model-0.877-0.088.h5\n",
      "\n",
      " -> Predicting at scale 32\n",
      " -> Predicting at scale 64\n",
      " -> Predicting at scale 128\n",
      " -> Predicting at scale 192\n",
      " -> Predicting at scale 256\n",
      " -> Predicting at scale 384\n",
      " -> Predicting at scale 448\n",
      " -> Predicting at scale 512\n",
      "\n",
      " -> Aggregating predictions\n",
      "\n",
      "-------------   Fold 5 / 5  -------------\n",
      "\n",
      " -> Loading weights from ../logs/2021-02-14/8/fold4/model-0.863-0.070.h5\n",
      "\n",
      " -> Predicting at scale 32\n",
      " -> Predicting at scale 64\n",
      " -> Predicting at scale 128\n",
      " -> Predicting at scale 192\n",
      " -> Predicting at scale 256\n",
      " -> Predicting at scale 320\n",
      " -> Predicting at scale 384\n",
      " -> Predicting at scale 512\n"
     ]
    }
   ],
   "source": [
    "for model_name, CP_FOLDER in zip(MODELS, CP_FOLDERS):\n",
    "    \n",
    "    print(f'\\n########################\\n -> Log folder : {CP_FOLDER}\\n########################\\n')\n",
    "    \n",
    "    save_folder = CP_FOLDER + \"preds/\"\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.mkdir(save_folder)\n",
    "\n",
    "    for fold_idx in range(5):\n",
    "        tf.keras.backend.clear_session()\n",
    "        print(f\"\\n-------------   Fold {fold_idx + 1} / {5}  -------------\\n\")\n",
    "\n",
    "        pred_agg, preds_multiscale = run_prediction(model_name, df_test, CP_FOLDER, fold_idx)\n",
    "\n",
    "        np.save(save_folder + f\"pred_agg_{fold_idx}.npy\", pred_agg)\n",
    "        for c, preds in enumerate(preds_multiscale):\n",
    "            np.save(save_folder + f\"pred_multiscale_{fold_idx}_{c}.npy\", preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds_to_pl(df_test, CP_FOLDER)\n",
    "np.save(save_folder + f\"pl_test.npy\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(os.listdir(TRAIN_MELS_PATH), columns=[\"recording_id\"])\n",
    "df_train[\"recording_id\"] = df_train[\"recording_id\"].apply(lambda x:x[:-4])\n",
    "\n",
    "df_tp = pd.read_csv(DATA_PATH + \"new_train_tp.csv\")\n",
    "recordings = list(np.unique(df_tp[\"recording_id\"].apply(lambda x:x.split('_')[0]).values))\n",
    "\n",
    "df_train[\"is_tp\"] = df_train[\"recording_id\"].apply(lambda x: x in recordings)\n",
    "df_train = df_train[df_train[\"is_tp\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_folder = CP_FOLDER + \"preds_train/\"\n",
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)\n",
    "\n",
    "for fold_idx in range(5):\n",
    "    print(f\"\\n-------------   Fold {fold_idx + 1} / {5}  -------------\\n\")\n",
    "    \n",
    "    pred_agg, preds_multiscale = run_prediction(df_train, CP_FOLDER, fold_idx, root=TRAIN_MELS_PATH)\n",
    "\n",
    "    np.save(save_folder + f\"pred_agg_{fold_idx}.npy\", pred_agg)\n",
    "    for c, preds in enumerate(preds_multiscale):\n",
    "        np.save(save_folder + f\"pred_multiscale_{fold_idx}_{c}.npy\", preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = preds_to_pl(df_train, CP_FOLDER, root='preds_train')\n",
    "np.save(save_folder + f\"pl_train.npy\", preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.plot(preds_train[i % 5][i])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

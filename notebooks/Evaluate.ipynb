{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/notebooks/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas\n",
    "except:\n",
    "    !pip install pandas iterative-stratification tensorflow_addons nlpaug==0.0.20 tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import click\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from train import get_model\n",
    "from split_data import get_split\n",
    "from metrics import LwlrapAccumulator\n",
    "from dataloader import MelSampler, convert_csv_to_dict_for_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "from utils.metric import lwlrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_scale_eval(checkpoints_path, fold, scales):\n",
    "    \"\"\"\n",
    "    This function will compute each species's lrap at different scale.\n",
    "    The idea is, for each species in test-set, we will use its predicted\n",
    "    value at the scale that maximize its lrap in eval set. in the case\n",
    "    that the maximum lrap value is achieved at many different scales, let take a\n",
    "    maximum scale because the smaller scale, the more false positive samples.\n",
    "    \"\"\"\n",
    "    train_data = pd.read_csv(\"../data/new_train_tp.csv\")\n",
    "    _, val_index = get_split(fold=fold)\n",
    "    valid_dataset_csv = train_data.iloc[val_index]\n",
    "\n",
    "    valid_data_loader = MelSampler(\n",
    "        convert_csv_to_dict_for_dataloader(valid_dataset_csv),\n",
    "        cache=True,\n",
    "        batch_size=64,\n",
    "        n_classes=24,\n",
    "        is_train=False,\n",
    "        use_cutmix=False,\n",
    "        shuffle_aug=False,\n",
    "        max_length=384,\n",
    "    )\n",
    "    all_checkpoints = sorted(\n",
    "        glob.glob(os.path.join(checkpoints_path, f\"fold{fold}\", \"model-*.h5\"))\n",
    "    )\n",
    "\n",
    "    model = get_model(\n",
    "        saved_path=checkpoints_path,\n",
    "        pretrained_with_contrastive=False,\n",
    "        pretrained_path=all_checkpoints[-1],\n",
    "    )\n",
    "\n",
    "    lwlrap_at_scale = np.zeros((len(scales), NUM_CLASSES))\n",
    "    preds = []\n",
    "\n",
    "    for s, max_length in enumerate(scales):\n",
    "        valid_data_loader.max_length = max_length\n",
    "        clip_preds = model.predict(valid_data_loader, verbose=0)  # [B, 24]\n",
    "        clip_preds = tf.nn.sigmoid(clip_preds).numpy()\n",
    "        preds.append(clip_preds)\n",
    "\n",
    "        # compute gts\n",
    "        gts = []\n",
    "        for i in range(len(valid_dataset_csv)):\n",
    "            gts.append(valid_dataset_csv.iloc[i][\"species_id\"])\n",
    "        gts = tf.keras.utils.to_categorical(gts, 24)\n",
    "\n",
    "        class_lwlrap, _ , score = lwlrap(gts, clip_preds)\n",
    "        \n",
    "        lwlrap_at_scale[s] = class_lwlrap\n",
    "        print(f\"Scale {scales[s]}:\\t{score:.3f}\")            \n",
    "\n",
    "    np.save(\n",
    "        os.path.join(checkpoints_path, \"lwlrap_at_scale_{fold}.npy\"),\n",
    "        lwlrap_at_scale,\n",
    "    )\n",
    "    \n",
    "    return lwlrap_at_scale, np.array(preds), gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALES = [32, 64, 128, 192, 256, 320, 384, 448, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP_FOLDER = LOG_PATH + \"2021-01-28/4/\"\n",
    "# CP_FOLDER = LOG_PATH + \"2021-01-28/8/\"\n",
    "CP_FOLDER = LOG_PATH + \"2021-01-28/10/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------   Fold 1 / 5  -------------\n",
      "\n",
      " -> Loading weights from ../logs/2021-01-28/10/fold0/model-0.864-0.487.h5\n",
      "\n",
      "Scale 32:\t0.566\n",
      "Scale 64:\t0.757\n",
      "Scale 128:\t0.851\n",
      "Scale 192:\t0.855\n",
      "Scale 256:\t0.858\n",
      "Scale 320:\t0.852\n",
      "Scale 384:\t0.842\n",
      "Scale 448:\t0.859\n",
      "Scale 512:\t0.831\n",
      "\n",
      "Multi-scale : 0.870\n",
      "\n",
      "-------------   Fold 2 / 5  -------------\n",
      "\n",
      " -> Loading weights from ../logs/2021-01-28/10/fold1/model-0.893-0.094.h5\n",
      "\n",
      "Scale 32:\t0.548\n",
      "Scale 64:\t0.752\n",
      "Scale 128:\t0.876\n",
      "Scale 192:\t0.895\n",
      "Scale 256:\t0.897\n",
      "Scale 320:\t0.899\n",
      "Scale 384:\t0.895\n",
      "Scale 448:\t0.890\n",
      "Scale 512:\t0.880\n",
      "\n",
      "Multi-scale : 0.924\n",
      "\n",
      "-------------   Fold 3 / 5  -------------\n",
      "\n",
      " -> Loading weights from ../logs/2021-01-28/10/fold2/model-0.851-0.054.h5\n",
      "\n",
      "Scale 32:\t0.484\n",
      "Scale 64:\t0.695\n",
      "Scale 128:\t0.815\n",
      "Scale 192:\t0.861\n",
      "Scale 256:\t0.859\n",
      "Scale 320:\t0.838\n",
      "Scale 384:\t0.837\n",
      "Scale 448:\t0.839\n",
      "Scale 512:\t0.841\n",
      "\n",
      "Multi-scale : 0.877\n",
      "\n",
      "-------------   Fold 4 / 5  -------------\n",
      "\n",
      " -> Loading weights from ../logs/2021-01-28/10/fold3/model-0.878-0.356.h5\n",
      "\n",
      "Scale 32:\t0.521\n",
      "Scale 64:\t0.725\n",
      "Scale 128:\t0.854\n",
      "Scale 192:\t0.875\n",
      "Scale 256:\t0.886\n",
      "Scale 320:\t0.868\n",
      "Scale 384:\t0.863\n",
      "Scale 448:\t0.856\n",
      "Scale 512:\t0.850\n",
      "\n",
      "Multi-scale : 0.913\n",
      "\n",
      "-------------   Fold 5 / 5  -------------\n",
      "\n",
      " -> Loading weights from ../logs/2021-01-28/10/fold4/model-0.868-0.078.h5\n",
      "\n",
      "Scale 32:\t0.386\n",
      "Scale 64:\t0.627\n",
      "Scale 128:\t0.792\n",
      "Scale 192:\t0.851\n",
      "Scale 256:\t0.855\n",
      "Scale 320:\t0.869\n",
      "Scale 384:\t0.867\n",
      "Scale 448:\t0.864\n",
      "Scale 512:\t0.861\n",
      "\n",
      "Multi-scale : 0.872\n"
     ]
    }
   ],
   "source": [
    "lwlraps = []\n",
    "preds = []\n",
    "best_preds = []\n",
    "ys = []\n",
    "\n",
    "for fold_idx in range(5):\n",
    "    print(f\"\\n-------------   Fold {fold_idx + 1} / {5}  -------------\\n\")\n",
    "    lwlrap_at_scale, pred, y = run_multi_scale_eval(CP_FOLDER, fold_idx, SCALES)\n",
    "    \n",
    "    best_pred = np.array([pred[lwlrap_at_scale[:, i].argmax(), :, i] for i in range(NUM_CLASSES)]).T\n",
    "    score = lwlrap(y, best_pred)[-1]\n",
    "\n",
    "    print(f'\\nMulti-scale : {score:.3f}')\n",
    "    \n",
    "    lwlraps.append(lwlrap_at_scale)\n",
    "    best_preds.append(best_pred)\n",
    "    preds.append(pred)\n",
    "    ys.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8976"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.892 + 0.933 + 0.888 + 0.872 + 0.903) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8946"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.875  +  0.925  + 0.889 +  0.915  + 0.869 ) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = lwlrap(np.concatenate(ys), np.concatenate(best_preds))[-1]\n",
    "\n",
    "print(f'Local CV score {score:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

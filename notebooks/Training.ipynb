{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/notebooks/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
      "Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.6/dist-packages (0.1.6)\n",
      "Requirement already satisfied: nlpaug==0.0.20 in /usr/local/lib/python3.6/dist-packages (0.0.20)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.56.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (7.1.2)\n",
      "Requirement already satisfied: tensorflow_probability==0.11.1 in /usr/local/lib/python3.6/dist-packages (0.11.1)\n",
      "Requirement already satisfied: tf2_resnets in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
      "Requirement already satisfied: tensorflow_addons==0.11.1 in /usr/local/lib/python3.6/dist-packages (0.11.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2020.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.5.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability==0.11.1) (1.6.0)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability==0.11.1) (0.1.5)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability==0.11.1) (1.15.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability==0.11.1) (4.4.2)\n",
      "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability==0.11.1) (0.3.3)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow_addons==0.11.1) (2.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (1.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas iterative-stratification nlpaug==0.0.20 tqdm click tensorflow_probability==0.11.1 tf2_resnets tensorflow_addons==0.11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataloader import BalancedMelSampler, MelSampler, convert_csv_to_dict_for_dataloader\n",
    "from losses import NpairsLoss\n",
    "from metrics import TFLWLRAP\n",
    "from split_data import get_split\n",
    "from train import get_model, get_callbacks, get_lr_metric\n",
    "from models import NUM_FRAMES, Classifier, DeepMetricLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "from utils.logger import prepare_log_folder, create_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "for i in range(len(physical_devices)):\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[i], True)\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(epoch, lr, min_lr=1e-5, max_lr=1e-3, epochs=30, warmup_prop=0.1):   \n",
    "    if epoch <= epochs * warmup_prop:\n",
    "        return min_lr + max_lr - (max_lr * (epochs * warmup_prop - epoch) / (epochs * warmup_prop))\n",
    "    else:\n",
    "        return min_lr + max_lr - (max_lr * (epoch - epochs * warmup_prop) / (epochs - epochs * warmup_prop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(fold_idx, saved_path, pretrained_path, pretrained_with_contrastive):\n",
    "    os.makedirs(os.path.join(saved_path, f\"fold{fold_idx}\"), exist_ok=True)\n",
    "    pretrained_with_contrastive = bool(pretrained_with_contrastive)\n",
    "\n",
    "    print(' -> Preparing Data \\n')\n",
    "    \n",
    "    train_data = pd.read_csv(\"../data/new_train_tp.csv\")\n",
    "    train_index, val_index = get_split(fold=fold_idx)\n",
    "    fold_train_dict = convert_csv_to_dict_for_dataloader(train_data.iloc[train_index])\n",
    "    fold_valid_dict = convert_csv_to_dict_for_dataloader(train_data.iloc[val_index])\n",
    "\n",
    "    balanced_train_data_loader = BalancedMelSampler(\n",
    "        fold_train_dict,\n",
    "        batch_size=32,\n",
    "        max_length=NUM_FRAMES,\n",
    "        is_train=True,\n",
    "        n_classes=24,\n",
    "        use_cutmix=True,\n",
    "        cache=True,\n",
    "        n_classes_in_batch=8,\n",
    "        shuffle_aug=False,\n",
    "    )\n",
    "\n",
    "    valid_data_loader = MelSampler(\n",
    "        fold_valid_dict,\n",
    "        batch_size=balanced_train_data_loader.batch_size,\n",
    "        n_classes=balanced_train_data_loader.n_classes,\n",
    "        cache=True,\n",
    "        max_length=NUM_FRAMES,\n",
    "        is_train=False,\n",
    "        use_cutmix=False,\n",
    "        shuffle_aug=balanced_train_data_loader.shuffle_aug,\n",
    "    )\n",
    "    \n",
    "#     return 0\n",
    "\n",
    "    print(' -> Preparing Model \\n')\n",
    "    model = get_model(\n",
    "        saved_path=saved_path,\n",
    "        pretrained_with_contrastive=pretrained_with_contrastive,\n",
    "        pretrained_path=pretrained_path,\n",
    "    )\n",
    "    model._build()\n",
    "    \n",
    "    scheduler = tfa.optimizers.Triangular2CyclicalLearningRate(\n",
    "        initial_learning_rate=1e-4,\n",
    "        maximal_learning_rate=1e-3,\n",
    "        step_size=15 * 20,\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(\n",
    "        tfa.optimizers.Lookahead(\n",
    "            tf.keras.optimizers.Adam(learning_rate=scheduler),\n",
    "            10,\n",
    "            0.5,\n",
    "        ),\n",
    "        \"dynamic\",\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        metrics=[TFLWLRAP(num_classes=24, name=\"lwlrap\")],\n",
    "        metric_loss_fn=NpairsLoss(temperature=0.1, name=\"n_pairs\"),\n",
    "        classification_loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    )   \n",
    "\n",
    "    print(' -> Training Model \\n')\n",
    "\n",
    "    callbacks = get_callbacks(pretrained_with_contrastive, fold_idx, saved_path=saved_path)\n",
    "    steps_per_epoch = int((len(fold_train_dict)) / balanced_train_data_loader.batch_size)\n",
    "    \n",
    "    model.fit(\n",
    "        balanced_train_data_loader,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=60,\n",
    "        validation_data=valid_data_loader,\n",
    "        callbacks=callbacks,\n",
    "        verbose=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "pretrained_folder = \"../logs/2021-01-28/1/\"   # simple\n",
    "pretrained_folder = \"../logs/2021-01-30/42/\"  # mixstyle + iunterpolate + cbam\n",
    "pretrained_folder = \"../logs/2021-01-31/8/\"  # mixstyle + iunterpolate + cbam without relu\n",
    "log_folder = \"../logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbones = [\n",
    "    \"efficientnetb0\",\n",
    "    \"efficientnetb1\",\n",
    "    \"efficientnetb2\",\n",
    "    \"efficientnetb3\",\n",
    "    \"efficientnetb4\",\n",
    "    \"resnet50\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging results to ../logs/2021-01-31/12/\n",
      "\n",
      "-------------   Fold 1 / 5  -------------\n",
      "\n",
      " -> Preparing Data \n",
      "\n",
      " -> Preparing Model \n",
      "\n",
      " -> Loading weights from ../logs/2021-01-31/8/pretrained_best_fold0.h5\n",
      "\n",
      " -> Training Model \n",
      "\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:574: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "30/30 - 10s - loss: 0.2135 - lwlrap: 0.4740 - val_loss: 0.1770 - val_lwlrap: 0.6641\n",
      "Epoch 2/60\n",
      "30/30 - 8s - loss: 0.1834 - lwlrap: 0.7522 - val_loss: 0.1476 - val_lwlrap: 0.7174\n",
      "Epoch 3/60\n",
      "30/30 - 8s - loss: 0.1559 - lwlrap: 0.7545 - val_loss: 0.1497 - val_lwlrap: 0.7327\n",
      "Epoch 4/60\n",
      "30/30 - 8s - loss: 0.1799 - lwlrap: 0.8399 - val_loss: 0.1470 - val_lwlrap: 0.7229\n",
      "Epoch 5/60\n",
      "30/30 - 8s - loss: 0.1201 - lwlrap: 0.8712 - val_loss: 0.1264 - val_lwlrap: 0.8138\n",
      "Epoch 6/60\n",
      "30/30 - 8s - loss: 0.1058 - lwlrap: 0.8619 - val_loss: 0.1363 - val_lwlrap: 0.6995\n",
      "Epoch 7/60\n",
      "30/30 - 8s - loss: 0.0965 - lwlrap: 0.8319 - val_loss: 0.1385 - val_lwlrap: 0.7214\n",
      "Epoch 8/60\n",
      "30/30 - 8s - loss: 0.0621 - lwlrap: 0.8491 - val_loss: 0.1111 - val_lwlrap: 0.7803\n",
      "Epoch 9/60\n",
      "30/30 - 8s - loss: 0.0897 - lwlrap: 0.8638 - val_loss: 0.1224 - val_lwlrap: 0.7691\n",
      "Epoch 10/60\n",
      "30/30 - 8s - loss: 0.1014 - lwlrap: 0.8744 - val_loss: 0.1569 - val_lwlrap: 0.7163\n",
      "Epoch 11/60\n",
      "30/30 - 8s - loss: 0.1202 - lwlrap: 0.8964 - val_loss: 0.1242 - val_lwlrap: 0.7856\n",
      "Epoch 12/60\n",
      "30/30 - 8s - loss: 0.0632 - lwlrap: 0.9203 - val_loss: 0.1521 - val_lwlrap: 0.7704\n",
      "Epoch 13/60\n",
      "30/30 - 8s - loss: 0.0457 - lwlrap: 0.9328 - val_loss: 0.0795 - val_lwlrap: 0.8116\n",
      "Epoch 14/60\n",
      "30/30 - 8s - loss: 0.0215 - lwlrap: 0.9611 - val_loss: 0.0660 - val_lwlrap: 0.8387\n",
      "Epoch 15/60\n",
      "30/30 - 8s - loss: 0.0224 - lwlrap: 0.9741 - val_loss: 0.1096 - val_lwlrap: 0.8197\n",
      "Epoch 16/60\n",
      "30/30 - 8s - loss: 0.0388 - lwlrap: 0.9700 - val_loss: 0.1053 - val_lwlrap: 0.8563\n",
      "Epoch 17/60\n",
      "30/30 - 8s - loss: 0.0346 - lwlrap: 0.9832 - val_loss: 0.1446 - val_lwlrap: 0.8345\n",
      "Epoch 18/60\n",
      "30/30 - 8s - loss: 0.0190 - lwlrap: 0.9834 - val_loss: 0.1218 - val_lwlrap: 0.8500\n",
      "Epoch 19/60\n",
      "30/30 - 8s - loss: 0.0085 - lwlrap: 0.9915 - val_loss: 0.1289 - val_lwlrap: 0.8315\n",
      "Epoch 20/60\n",
      "30/30 - 8s - loss: 0.0129 - lwlrap: 0.9945 - val_loss: 0.1257 - val_lwlrap: 0.8607\n",
      "Epoch 21/60\n",
      "30/30 - 8s - loss: 0.0051 - lwlrap: 0.9963 - val_loss: 0.1189 - val_lwlrap: 0.8565\n",
      "Epoch 22/60\n",
      "30/30 - 8s - loss: 0.0037 - lwlrap: 0.9959 - val_loss: 0.1267 - val_lwlrap: 0.8608\n",
      "Epoch 23/60\n",
      "30/30 - 8s - loss: 0.0110 - lwlrap: 0.9968 - val_loss: 0.1410 - val_lwlrap: 0.8483\n",
      "Epoch 24/60\n",
      "30/30 - 8s - loss: 0.0048 - lwlrap: 0.9972 - val_loss: 0.1395 - val_lwlrap: 0.8495\n",
      "Epoch 25/60\n",
      "30/30 - 8s - loss: 0.0159 - lwlrap: 0.9954 - val_loss: 0.1077 - val_lwlrap: 0.8444\n",
      "Epoch 26/60\n",
      "30/30 - 8s - loss: 0.0095 - lwlrap: 0.9971 - val_loss: 0.1720 - val_lwlrap: 0.8522\n",
      "Epoch 27/60\n",
      "30/30 - 8s - loss: 0.0165 - lwlrap: 0.9978 - val_loss: 0.1533 - val_lwlrap: 0.8375\n",
      "Epoch 28/60\n",
      "30/30 - 8s - loss: 0.0075 - lwlrap: 0.9884 - val_loss: 0.1201 - val_lwlrap: 0.8563\n",
      "Epoch 29/60\n",
      "30/30 - 8s - loss: 0.0863 - lwlrap: 0.9890 - val_loss: 0.1432 - val_lwlrap: 0.8071\n",
      "Epoch 30/60\n",
      "30/30 - 8s - loss: 0.0166 - lwlrap: 0.9730 - val_loss: 0.1383 - val_lwlrap: 0.8153\n",
      "Epoch 31/60\n",
      "30/30 - 8s - loss: 0.0261 - lwlrap: 0.9729 - val_loss: 0.1194 - val_lwlrap: 0.8572\n",
      "Epoch 32/60\n",
      "30/30 - 8s - loss: 0.0091 - lwlrap: 0.9878 - val_loss: 0.1315 - val_lwlrap: 0.8065\n",
      "Epoch 33/60\n",
      "30/30 - 8s - loss: 0.0032 - lwlrap: 0.9931 - val_loss: 0.1373 - val_lwlrap: 0.8270\n",
      "Epoch 34/60\n",
      "30/30 - 8s - loss: 0.0096 - lwlrap: 0.9965 - val_loss: 0.1007 - val_lwlrap: 0.8542\n",
      "Epoch 35/60\n",
      "30/30 - 8s - loss: 0.0109 - lwlrap: 0.9975 - val_loss: 0.1142 - val_lwlrap: 0.8689\n",
      "Epoch 36/60\n",
      "30/30 - 8s - loss: 0.0380 - lwlrap: 0.9934 - val_loss: 0.1218 - val_lwlrap: 0.8491\n",
      "Epoch 37/60\n",
      "30/30 - 8s - loss: 0.0028 - lwlrap: 0.9987 - val_loss: 0.1274 - val_lwlrap: 0.8632\n",
      "Epoch 38/60\n",
      "30/30 - 8s - loss: 0.0070 - lwlrap: 0.9989 - val_loss: 0.1423 - val_lwlrap: 0.8451\n",
      "Epoch 39/60\n",
      "30/30 - 8s - loss: 0.0030 - lwlrap: 0.9989 - val_loss: 0.1337 - val_lwlrap: 0.8311\n",
      "Epoch 40/60\n",
      "30/30 - 8s - loss: 0.0070 - lwlrap: 0.9976 - val_loss: 0.1453 - val_lwlrap: 0.8430\n",
      "Epoch 41/60\n",
      "30/30 - 8s - loss: 0.0015 - lwlrap: 0.9981 - val_loss: 0.1415 - val_lwlrap: 0.8584\n",
      "Epoch 42/60\n",
      "30/30 - 8s - loss: 0.0028 - lwlrap: 1.0000 - val_loss: 0.1386 - val_lwlrap: 0.8390\n",
      "Epoch 43/60\n",
      "30/30 - 8s - loss: 0.0017 - lwlrap: 0.9994 - val_loss: 0.1115 - val_lwlrap: 0.8532\n",
      "Epoch 44/60\n",
      "30/30 - 8s - loss: 0.0128 - lwlrap: 0.9990 - val_loss: 0.1501 - val_lwlrap: 0.8389\n",
      "Epoch 45/60\n",
      "30/30 - 8s - loss: 0.0018 - lwlrap: 0.9990 - val_loss: 0.1438 - val_lwlrap: 0.8477\n",
      "Epoch 46/60\n",
      "30/30 - 8s - loss: 0.0017 - lwlrap: 0.9998 - val_loss: 0.1208 - val_lwlrap: 0.8569\n",
      "Epoch 47/60\n",
      "30/30 - 8s - loss: 0.0031 - lwlrap: 0.9995 - val_loss: 0.1194 - val_lwlrap: 0.8607\n",
      "Epoch 48/60\n",
      "30/30 - 8s - loss: 0.0032 - lwlrap: 0.9970 - val_loss: 0.1185 - val_lwlrap: 0.8326\n",
      "Epoch 49/60\n",
      "30/30 - 8s - loss: 0.0134 - lwlrap: 0.9976 - val_loss: 0.1617 - val_lwlrap: 0.8360\n",
      "Epoch 50/60\n",
      "30/30 - 8s - loss: 0.0013 - lwlrap: 0.9998 - val_loss: 0.1538 - val_lwlrap: 0.8437\n",
      "Epoch 51/60\n",
      "30/30 - 8s - loss: 0.0022 - lwlrap: 0.9988 - val_loss: 0.1798 - val_lwlrap: 0.8584\n",
      "Epoch 52/60\n",
      "30/30 - 8s - loss: 0.0149 - lwlrap: 0.9986 - val_loss: 0.1609 - val_lwlrap: 0.8368\n",
      "Epoch 53/60\n",
      "30/30 - 8s - loss: 0.0127 - lwlrap: 0.9981 - val_loss: 0.1526 - val_lwlrap: 0.8197\n",
      "Epoch 54/60\n",
      "30/30 - 8s - loss: 0.0074 - lwlrap: 0.9985 - val_loss: 0.1402 - val_lwlrap: 0.8611\n",
      "Epoch 55/60\n",
      "30/30 - 8s - loss: 9.8368e-04 - lwlrap: 0.9996 - val_loss: 0.1130 - val_lwlrap: 0.8615\n",
      "\n",
      "-------------   Fold 2 / 5  -------------\n",
      "\n",
      " -> Preparing Data \n",
      "\n",
      " -> Preparing Model \n",
      "\n",
      " -> Loading weights from ../logs/2021-01-31/8/pretrained_best_fold1.h5\n",
      "\n",
      " -> Training Model \n",
      "\n",
      "Epoch 1/60\n",
      "30/30 - 10s - loss: 0.0904 - lwlrap: 0.4370 - val_loss: 0.1224 - val_lwlrap: 0.6834\n",
      "Epoch 2/60\n",
      "30/30 - 9s - loss: 0.1212 - lwlrap: 0.7922 - val_loss: 0.0934 - val_lwlrap: 0.7866\n",
      "Epoch 3/60\n",
      "30/30 - 9s - loss: 0.0671 - lwlrap: 0.8827 - val_loss: 0.0849 - val_lwlrap: 0.8439\n",
      "Epoch 4/60\n",
      "30/30 - 8s - loss: 0.0445 - lwlrap: 0.9172 - val_loss: 0.0718 - val_lwlrap: 0.8457\n",
      "Epoch 5/60\n",
      "30/30 - 8s - loss: 0.0882 - lwlrap: 0.9080 - val_loss: 0.0853 - val_lwlrap: 0.8240\n",
      "Epoch 6/60\n",
      "30/30 - 8s - loss: 0.0677 - lwlrap: 0.9088 - val_loss: 0.0809 - val_lwlrap: 0.7995\n",
      "Epoch 7/60\n",
      "30/30 - 8s - loss: 0.1090 - lwlrap: 0.8840 - val_loss: 0.0817 - val_lwlrap: 0.7817\n",
      "Epoch 8/60\n",
      "30/30 - 8s - loss: 0.0576 - lwlrap: 0.9028 - val_loss: 0.0750 - val_lwlrap: 0.8540\n",
      "Epoch 9/60\n",
      "30/30 - 8s - loss: 0.0610 - lwlrap: 0.9277 - val_loss: 0.0559 - val_lwlrap: 0.8492\n",
      "Epoch 10/60\n",
      "30/30 - 8s - loss: 0.0918 - lwlrap: 0.9381 - val_loss: 0.0697 - val_lwlrap: 0.8573\n",
      "Epoch 11/60\n",
      "30/30 - 8s - loss: 0.0573 - lwlrap: 0.9298 - val_loss: 0.0525 - val_lwlrap: 0.8632\n",
      "Epoch 12/60\n",
      "30/30 - 8s - loss: 0.0472 - lwlrap: 0.9532 - val_loss: 0.0917 - val_lwlrap: 0.8529\n",
      "Epoch 13/60\n",
      "30/30 - 8s - loss: 0.0373 - lwlrap: 0.9611 - val_loss: 0.0549 - val_lwlrap: 0.8772\n",
      "Epoch 14/60\n",
      "30/30 - 8s - loss: 0.0171 - lwlrap: 0.9715 - val_loss: 0.0703 - val_lwlrap: 0.8494\n",
      "Epoch 15/60\n",
      "30/30 - 8s - loss: 0.0680 - lwlrap: 0.9876 - val_loss: 0.0735 - val_lwlrap: 0.8256\n",
      "Epoch 16/60\n",
      "30/30 - 8s - loss: 0.0114 - lwlrap: 0.9861 - val_loss: 0.0949 - val_lwlrap: 0.8708\n",
      "Epoch 17/60\n",
      "30/30 - 8s - loss: 0.0081 - lwlrap: 0.9932 - val_loss: 0.0870 - val_lwlrap: 0.9000\n",
      "Epoch 18/60\n",
      "30/30 - 8s - loss: 0.0277 - lwlrap: 0.9943 - val_loss: 0.0631 - val_lwlrap: 0.8969\n",
      "Epoch 19/60\n",
      "30/30 - 8s - loss: 0.0072 - lwlrap: 0.9948 - val_loss: 0.0503 - val_lwlrap: 0.8923\n",
      "Epoch 20/60\n",
      "30/30 - 8s - loss: 0.0046 - lwlrap: 0.9948 - val_loss: 0.0510 - val_lwlrap: 0.9086\n",
      "Epoch 21/60\n",
      "30/30 - 8s - loss: 0.0346 - lwlrap: 0.9950 - val_loss: 0.0547 - val_lwlrap: 0.8978\n",
      "Epoch 22/60\n",
      "30/30 - 8s - loss: 0.0125 - lwlrap: 0.9984 - val_loss: 0.0405 - val_lwlrap: 0.8982\n",
      "Epoch 23/60\n",
      "30/30 - 8s - loss: 0.0027 - lwlrap: 0.9956 - val_loss: 0.0538 - val_lwlrap: 0.8924\n",
      "Epoch 24/60\n",
      "30/30 - 8s - loss: 0.0039 - lwlrap: 0.9989 - val_loss: 0.0686 - val_lwlrap: 0.8978\n",
      "Epoch 25/60\n",
      "30/30 - 8s - loss: 0.0090 - lwlrap: 0.9979 - val_loss: 0.0401 - val_lwlrap: 0.9231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/60\n",
      "30/30 - 8s - loss: 0.0142 - lwlrap: 0.9952 - val_loss: 0.0500 - val_lwlrap: 0.9011\n",
      "Epoch 27/60\n",
      "30/30 - 8s - loss: 0.0109 - lwlrap: 0.9904 - val_loss: 0.0586 - val_lwlrap: 0.8956\n",
      "Epoch 28/60\n",
      "30/30 - 8s - loss: 0.0316 - lwlrap: 0.9931 - val_loss: 0.0726 - val_lwlrap: 0.8764\n",
      "Epoch 29/60\n",
      "30/30 - 8s - loss: 0.0040 - lwlrap: 0.9955 - val_loss: 0.0676 - val_lwlrap: 0.8817\n",
      "Epoch 30/60\n",
      "30/30 - 8s - loss: 0.0058 - lwlrap: 0.9947 - val_loss: 0.0581 - val_lwlrap: 0.8806\n",
      "Epoch 31/60\n",
      "30/30 - 8s - loss: 0.0020 - lwlrap: 0.9975 - val_loss: 0.0513 - val_lwlrap: 0.8975\n",
      "Epoch 32/60\n",
      "30/30 - 8s - loss: 0.0111 - lwlrap: 0.9974 - val_loss: 0.0652 - val_lwlrap: 0.8739\n",
      "Epoch 33/60\n",
      "30/30 - 8s - loss: 0.0063 - lwlrap: 0.9935 - val_loss: 0.0712 - val_lwlrap: 0.8627\n",
      "Epoch 34/60\n",
      "30/30 - 8s - loss: 0.0012 - lwlrap: 0.9973 - val_loss: 0.0542 - val_lwlrap: 0.8816\n",
      "Epoch 35/60\n",
      "30/30 - 8s - loss: 0.0123 - lwlrap: 0.9970 - val_loss: 0.0646 - val_lwlrap: 0.8719\n",
      "Epoch 36/60\n",
      "30/30 - 8s - loss: 0.0046 - lwlrap: 0.9967 - val_loss: 0.0443 - val_lwlrap: 0.8818\n",
      "Epoch 37/60\n",
      "30/30 - 8s - loss: 0.0038 - lwlrap: 0.9977 - val_loss: 0.0445 - val_lwlrap: 0.8838\n",
      "Epoch 38/60\n",
      "30/30 - 8s - loss: 0.0057 - lwlrap: 0.9993 - val_loss: 0.0332 - val_lwlrap: 0.9026\n",
      "Epoch 39/60\n",
      "30/30 - 8s - loss: 0.0025 - lwlrap: 0.9988 - val_loss: 0.0340 - val_lwlrap: 0.9031\n",
      "Epoch 40/60\n",
      "30/30 - 8s - loss: 6.4144e-04 - lwlrap: 1.0000 - val_loss: 0.0336 - val_lwlrap: 0.8869\n",
      "Epoch 41/60\n",
      "30/30 - 8s - loss: 9.1816e-04 - lwlrap: 0.9998 - val_loss: 0.0470 - val_lwlrap: 0.8973\n",
      "Epoch 42/60\n",
      "30/30 - 8s - loss: 0.0040 - lwlrap: 0.9987 - val_loss: 0.0379 - val_lwlrap: 0.9072\n",
      "Epoch 43/60\n",
      "30/30 - 8s - loss: 9.9806e-04 - lwlrap: 0.9995 - val_loss: 0.0469 - val_lwlrap: 0.9145\n",
      "Epoch 44/60\n",
      "30/30 - 8s - loss: 0.0128 - lwlrap: 0.9995 - val_loss: 0.0595 - val_lwlrap: 0.9092\n",
      "Epoch 45/60\n",
      "30/30 - 8s - loss: 0.0039 - lwlrap: 0.9990 - val_loss: 0.0587 - val_lwlrap: 0.9000\n",
      "\n",
      "-------------   Fold 3 / 5  -------------\n",
      "\n",
      " -> Preparing Data \n",
      "\n",
      " -> Preparing Model \n",
      "\n",
      " -> Loading weights from ../logs/2021-01-31/8/pretrained_best_fold2.h5\n",
      "\n",
      " -> Training Model \n",
      "\n",
      "Epoch 1/60\n",
      "30/30 - 10s - loss: 0.1322 - lwlrap: 0.4567 - val_loss: 0.1449 - val_lwlrap: 0.6254\n",
      "Epoch 2/60\n",
      "30/30 - 8s - loss: 0.1104 - lwlrap: 0.7451 - val_loss: 0.1246 - val_lwlrap: 0.7747\n",
      "Epoch 3/60\n",
      "30/30 - 9s - loss: 0.1178 - lwlrap: 0.8559 - val_loss: 0.0979 - val_lwlrap: 0.7774\n",
      "Epoch 4/60\n",
      "30/30 - 8s - loss: 0.0926 - lwlrap: 0.8833 - val_loss: 0.0905 - val_lwlrap: 0.7793\n",
      "Epoch 5/60\n",
      "30/30 - 8s - loss: 0.0787 - lwlrap: 0.8714 - val_loss: 0.0789 - val_lwlrap: 0.8122\n",
      "Epoch 6/60\n",
      "30/30 - 8s - loss: 0.0810 - lwlrap: 0.8870 - val_loss: 0.0814 - val_lwlrap: 0.7502\n",
      "Epoch 7/60\n",
      "30/30 - 8s - loss: 0.2187 - lwlrap: 0.8712 - val_loss: 0.0968 - val_lwlrap: 0.7413\n",
      "Epoch 8/60\n",
      "30/30 - 8s - loss: 0.1238 - lwlrap: 0.8739 - val_loss: 0.0799 - val_lwlrap: 0.7867\n",
      "Epoch 9/60\n",
      "30/30 - 8s - loss: 0.0696 - lwlrap: 0.8814 - val_loss: 0.0946 - val_lwlrap: 0.7640\n",
      "Epoch 10/60\n",
      "30/30 - 8s - loss: 0.0461 - lwlrap: 0.8898 - val_loss: 0.0846 - val_lwlrap: 0.7560\n",
      "Epoch 11/60\n",
      "30/30 - 8s - loss: 0.0518 - lwlrap: 0.9059 - val_loss: 0.0634 - val_lwlrap: 0.7947\n",
      "Epoch 12/60\n",
      "30/30 - 8s - loss: 0.0452 - lwlrap: 0.9356 - val_loss: 0.0687 - val_lwlrap: 0.8114\n",
      "Epoch 13/60\n",
      "30/30 - 8s - loss: 0.0930 - lwlrap: 0.9333 - val_loss: 0.0648 - val_lwlrap: 0.7456\n",
      "Epoch 14/60\n",
      "30/30 - 8s - loss: 0.0459 - lwlrap: 0.9457 - val_loss: 0.0586 - val_lwlrap: 0.8244\n",
      "Epoch 15/60\n",
      "30/30 - 8s - loss: 0.0328 - lwlrap: 0.9679 - val_loss: 0.0506 - val_lwlrap: 0.8131\n",
      "Epoch 16/60\n",
      "30/30 - 8s - loss: 0.0212 - lwlrap: 0.9793 - val_loss: 0.0412 - val_lwlrap: 0.8227\n",
      "Epoch 17/60\n",
      "30/30 - 8s - loss: 0.0167 - lwlrap: 0.9842 - val_loss: 0.0304 - val_lwlrap: 0.8414\n",
      "Epoch 18/60\n",
      "30/30 - 8s - loss: 0.0161 - lwlrap: 0.9913 - val_loss: 0.0330 - val_lwlrap: 0.8486\n",
      "Epoch 19/60\n",
      "30/30 - 8s - loss: 0.0092 - lwlrap: 0.9931 - val_loss: 0.0334 - val_lwlrap: 0.8461\n",
      "Epoch 20/60\n",
      "30/30 - 8s - loss: 0.0049 - lwlrap: 0.9951 - val_loss: 0.0361 - val_lwlrap: 0.8433\n",
      "Epoch 21/60\n",
      "30/30 - 8s - loss: 0.0072 - lwlrap: 0.9937 - val_loss: 0.0361 - val_lwlrap: 0.8588\n",
      "Epoch 22/60\n",
      "30/30 - 8s - loss: 0.0063 - lwlrap: 0.9959 - val_loss: 0.0370 - val_lwlrap: 0.8415\n",
      "Epoch 23/60\n",
      "30/30 - 8s - loss: 0.0254 - lwlrap: 0.9954 - val_loss: 0.0345 - val_lwlrap: 0.8604\n",
      "Epoch 24/60\n",
      "30/30 - 8s - loss: 0.0090 - lwlrap: 0.9953 - val_loss: 0.0386 - val_lwlrap: 0.8422\n",
      "Epoch 25/60\n",
      "30/30 - 8s - loss: 0.0128 - lwlrap: 0.9949 - val_loss: 0.0334 - val_lwlrap: 0.8445\n",
      "Epoch 26/60\n",
      "30/30 - 8s - loss: 0.0157 - lwlrap: 0.9911 - val_loss: 0.0395 - val_lwlrap: 0.8401\n",
      "Epoch 27/60\n",
      "30/30 - 8s - loss: 0.0553 - lwlrap: 0.9942 - val_loss: 0.0355 - val_lwlrap: 0.8600\n",
      "Epoch 28/60\n",
      "30/30 - 8s - loss: 0.0097 - lwlrap: 0.9929 - val_loss: 0.0382 - val_lwlrap: 0.8462\n",
      "Epoch 29/60\n",
      "30/30 - 8s - loss: 0.0049 - lwlrap: 0.9892 - val_loss: 0.0603 - val_lwlrap: 0.8179\n",
      "Epoch 30/60\n",
      "30/30 - 8s - loss: 0.0252 - lwlrap: 0.9774 - val_loss: 0.0456 - val_lwlrap: 0.8247\n",
      "Epoch 31/60\n",
      "30/30 - 8s - loss: 0.0238 - lwlrap: 0.9904 - val_loss: 0.0364 - val_lwlrap: 0.8449\n",
      "Epoch 32/60\n",
      "30/30 - 8s - loss: 0.0030 - lwlrap: 0.9970 - val_loss: 0.0509 - val_lwlrap: 0.8353\n",
      "Epoch 33/60\n",
      "30/30 - 8s - loss: 0.0398 - lwlrap: 0.9900 - val_loss: 0.0437 - val_lwlrap: 0.8339\n",
      "Epoch 34/60\n",
      "30/30 - 8s - loss: 0.0039 - lwlrap: 0.9934 - val_loss: 0.0504 - val_lwlrap: 0.8427\n",
      "Epoch 35/60\n",
      "30/30 - 8s - loss: 0.0138 - lwlrap: 0.9945 - val_loss: 0.0398 - val_lwlrap: 0.8519\n",
      "Epoch 36/60\n",
      "30/30 - 8s - loss: 0.0022 - lwlrap: 0.9970 - val_loss: 0.0302 - val_lwlrap: 0.8516\n",
      "Epoch 37/60\n",
      "30/30 - 8s - loss: 0.0040 - lwlrap: 0.9978 - val_loss: 0.0349 - val_lwlrap: 0.8513\n",
      "Epoch 38/60\n",
      "30/30 - 8s - loss: 0.0068 - lwlrap: 0.9985 - val_loss: 0.0503 - val_lwlrap: 0.8401\n",
      "Epoch 39/60\n",
      "30/30 - 8s - loss: 0.0028 - lwlrap: 0.9974 - val_loss: 0.0450 - val_lwlrap: 0.8544\n",
      "Epoch 40/60\n",
      "30/30 - 8s - loss: 0.0066 - lwlrap: 0.9970 - val_loss: 0.0463 - val_lwlrap: 0.8490\n",
      "Epoch 41/60\n",
      "30/30 - 8s - loss: 0.0028 - lwlrap: 0.9986 - val_loss: 0.0406 - val_lwlrap: 0.8573\n",
      "Epoch 42/60\n",
      "30/30 - 9s - loss: 0.0030 - lwlrap: 0.9994 - val_loss: 0.0456 - val_lwlrap: 0.8623\n",
      "Epoch 43/60\n",
      "30/30 - 8s - loss: 0.0024 - lwlrap: 0.9997 - val_loss: 0.0357 - val_lwlrap: 0.8536\n",
      "Epoch 44/60\n",
      "30/30 - 8s - loss: 0.0026 - lwlrap: 0.9983 - val_loss: 0.0436 - val_lwlrap: 0.8507\n",
      "Epoch 45/60\n",
      "30/30 - 8s - loss: 0.0025 - lwlrap: 0.9994 - val_loss: 0.0424 - val_lwlrap: 0.8549\n",
      "Epoch 46/60\n",
      "30/30 - 8s - loss: 0.0031 - lwlrap: 0.9991 - val_loss: 0.0494 - val_lwlrap: 0.8575\n",
      "Epoch 47/60\n",
      "30/30 - 8s - loss: 0.0025 - lwlrap: 0.9989 - val_loss: 0.0484 - val_lwlrap: 0.8444\n",
      "Epoch 48/60\n",
      "30/30 - 8s - loss: 0.0065 - lwlrap: 0.9976 - val_loss: 0.0386 - val_lwlrap: 0.8541\n",
      "Epoch 49/60\n",
      "30/30 - 8s - loss: 0.0017 - lwlrap: 0.9987 - val_loss: 0.0433 - val_lwlrap: 0.8403\n",
      "Epoch 50/60\n",
      "30/30 - 8s - loss: 0.0029 - lwlrap: 0.9975 - val_loss: 0.0661 - val_lwlrap: 0.8329\n",
      "Epoch 51/60\n",
      "30/30 - 8s - loss: 0.0016 - lwlrap: 0.9983 - val_loss: 0.0488 - val_lwlrap: 0.8372\n",
      "Epoch 52/60\n",
      "30/30 - 8s - loss: 0.0058 - lwlrap: 0.9966 - val_loss: 0.0488 - val_lwlrap: 0.8430\n",
      "Epoch 53/60\n",
      "30/30 - 8s - loss: 0.0038 - lwlrap: 0.9978 - val_loss: 0.0483 - val_lwlrap: 0.8399\n",
      "Epoch 54/60\n",
      "30/30 - 8s - loss: 0.0027 - lwlrap: 0.9990 - val_loss: 0.0434 - val_lwlrap: 0.8365\n",
      "Epoch 55/60\n",
      "30/30 - 8s - loss: 9.7840e-04 - lwlrap: 0.9986 - val_loss: 0.0400 - val_lwlrap: 0.8422\n",
      "Epoch 56/60\n",
      "30/30 - 8s - loss: 0.0025 - lwlrap: 0.9993 - val_loss: 0.0328 - val_lwlrap: 0.8591\n",
      "Epoch 57/60\n",
      "30/30 - 8s - loss: 0.0016 - lwlrap: 0.9993 - val_loss: 0.0408 - val_lwlrap: 0.8304\n",
      "Epoch 58/60\n",
      "30/30 - 8s - loss: 0.0073 - lwlrap: 0.9985 - val_loss: 0.0397 - val_lwlrap: 0.8384\n",
      "Epoch 59/60\n",
      "30/30 - 8s - loss: 0.0022 - lwlrap: 0.9996 - val_loss: 0.0488 - val_lwlrap: 0.8637\n",
      "Epoch 60/60\n",
      "30/30 - 8s - loss: 0.0016 - lwlrap: 0.9994 - val_loss: 0.0382 - val_lwlrap: 0.8465\n",
      "\n",
      "-------------   Fold 4 / 5  -------------\n",
      "\n",
      " -> Preparing Data \n",
      "\n",
      " -> Preparing Model \n",
      "\n",
      " -> Loading weights from ../logs/2021-01-31/8/pretrained_best_fold3.h5\n",
      "\n",
      " -> Training Model \n",
      "\n",
      "Epoch 1/60\n",
      "30/30 - 10s - loss: 0.1288 - lwlrap: 0.4973 - val_loss: 0.1297 - val_lwlrap: 0.6273\n",
      "Epoch 2/60\n",
      "30/30 - 8s - loss: 0.1367 - lwlrap: 0.7466 - val_loss: 0.1187 - val_lwlrap: 0.7294\n",
      "Epoch 3/60\n",
      "30/30 - 8s - loss: 0.1245 - lwlrap: 0.8615 - val_loss: 0.0927 - val_lwlrap: 0.8190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/60\n",
      "30/30 - 8s - loss: 0.1093 - lwlrap: 0.8964 - val_loss: 0.0884 - val_lwlrap: 0.8020\n",
      "Epoch 5/60\n",
      "30/30 - 8s - loss: 0.0561 - lwlrap: 0.9113 - val_loss: 0.1079 - val_lwlrap: 0.7903\n",
      "Epoch 6/60\n",
      "30/30 - 8s - loss: 0.1618 - lwlrap: 0.8821 - val_loss: 0.1108 - val_lwlrap: 0.7397\n",
      "Epoch 7/60\n",
      "30/30 - 8s - loss: 0.0688 - lwlrap: 0.8535 - val_loss: 0.1107 - val_lwlrap: 0.7509\n",
      "Epoch 8/60\n",
      "30/30 - 9s - loss: 0.0749 - lwlrap: 0.8864 - val_loss: 0.0597 - val_lwlrap: 0.8478\n",
      "Epoch 9/60\n",
      "30/30 - 8s - loss: 0.0533 - lwlrap: 0.9051 - val_loss: 0.0903 - val_lwlrap: 0.7359\n",
      "Epoch 10/60\n",
      "30/30 - 8s - loss: 0.1746 - lwlrap: 0.9020 - val_loss: 0.0862 - val_lwlrap: 0.7874\n",
      "Epoch 11/60\n",
      "30/30 - 8s - loss: 0.0706 - lwlrap: 0.8789 - val_loss: 0.1039 - val_lwlrap: 0.6926\n",
      "Epoch 12/60\n",
      "30/30 - 8s - loss: 0.0457 - lwlrap: 0.9015 - val_loss: 0.0613 - val_lwlrap: 0.8058\n",
      "Epoch 13/60\n",
      "30/30 - 8s - loss: 0.0317 - lwlrap: 0.9362 - val_loss: 0.1346 - val_lwlrap: 0.7417\n",
      "Epoch 14/60\n",
      "30/30 - 8s - loss: 0.0231 - lwlrap: 0.9711 - val_loss: 0.0381 - val_lwlrap: 0.8287\n",
      "Epoch 15/60\n",
      "30/30 - 8s - loss: 0.0236 - lwlrap: 0.9785 - val_loss: 0.0347 - val_lwlrap: 0.8713\n",
      "Epoch 16/60\n",
      "30/30 - 8s - loss: 0.0187 - lwlrap: 0.9873 - val_loss: 0.0264 - val_lwlrap: 0.8885\n",
      "Epoch 17/60\n",
      "30/30 - 8s - loss: 0.0275 - lwlrap: 0.9831 - val_loss: 0.0424 - val_lwlrap: 0.8734\n",
      "Epoch 18/60\n",
      "30/30 - 8s - loss: 0.0117 - lwlrap: 0.9938 - val_loss: 0.0292 - val_lwlrap: 0.8919\n",
      "Epoch 19/60\n",
      "30/30 - 8s - loss: 0.0189 - lwlrap: 0.9952 - val_loss: 0.0332 - val_lwlrap: 0.8881\n",
      "Epoch 20/60\n",
      "30/30 - 8s - loss: 0.0101 - lwlrap: 0.9931 - val_loss: 0.0318 - val_lwlrap: 0.8905\n",
      "Epoch 21/60\n",
      "30/30 - 8s - loss: 0.0038 - lwlrap: 0.9974 - val_loss: 0.0353 - val_lwlrap: 0.8868\n",
      "Epoch 22/60\n",
      "30/30 - 8s - loss: 0.0077 - lwlrap: 0.9921 - val_loss: 0.0384 - val_lwlrap: 0.8897\n",
      "Epoch 23/60\n",
      "30/30 - 8s - loss: 0.0049 - lwlrap: 0.9969 - val_loss: 0.0204 - val_lwlrap: 0.8863\n",
      "Epoch 24/60\n",
      "30/30 - 8s - loss: 0.0030 - lwlrap: 0.9961 - val_loss: 0.0244 - val_lwlrap: 0.8928\n",
      "Epoch 25/60\n",
      "30/30 - 8s - loss: 0.0108 - lwlrap: 0.9931 - val_loss: 0.0399 - val_lwlrap: 0.8916\n",
      "Epoch 26/60\n",
      "30/30 - 8s - loss: 0.0504 - lwlrap: 0.9917 - val_loss: 0.0237 - val_lwlrap: 0.8914\n",
      "Epoch 27/60\n",
      "30/30 - 8s - loss: 0.0054 - lwlrap: 0.9928 - val_loss: 0.0435 - val_lwlrap: 0.8873\n",
      "Epoch 28/60\n",
      "30/30 - 8s - loss: 0.0302 - lwlrap: 0.9895 - val_loss: 0.0267 - val_lwlrap: 0.8378\n",
      "Epoch 29/60\n",
      "30/30 - 8s - loss: 0.0146 - lwlrap: 0.9910 - val_loss: 0.0543 - val_lwlrap: 0.8379\n",
      "Epoch 30/60\n",
      "30/30 - 8s - loss: 0.0404 - lwlrap: 0.9840 - val_loss: 0.0423 - val_lwlrap: 0.8531\n",
      "Epoch 31/60\n",
      "30/30 - 8s - loss: 0.0120 - lwlrap: 0.9852 - val_loss: 0.0368 - val_lwlrap: 0.8544\n",
      "Epoch 32/60\n",
      "30/30 - 8s - loss: 0.0288 - lwlrap: 0.9779 - val_loss: 0.0400 - val_lwlrap: 0.8587\n",
      "Epoch 33/60\n",
      "30/30 - 8s - loss: 0.0240 - lwlrap: 0.9908 - val_loss: 0.0384 - val_lwlrap: 0.8798\n",
      "Epoch 34/60\n",
      "30/30 - 8s - loss: 0.0028 - lwlrap: 0.9957 - val_loss: 0.0198 - val_lwlrap: 0.8845\n",
      "Epoch 35/60\n",
      "30/30 - 8s - loss: 0.0034 - lwlrap: 0.9983 - val_loss: 0.0396 - val_lwlrap: 0.8735\n",
      "Epoch 36/60\n",
      "30/30 - 8s - loss: 0.0047 - lwlrap: 0.9957 - val_loss: 0.0271 - val_lwlrap: 0.8772\n",
      "Epoch 37/60\n",
      "30/30 - 8s - loss: 0.0043 - lwlrap: 0.9988 - val_loss: 0.0299 - val_lwlrap: 0.8862\n",
      "Epoch 38/60\n",
      "30/30 - 8s - loss: 0.0028 - lwlrap: 0.9989 - val_loss: 0.0214 - val_lwlrap: 0.8897\n",
      "Epoch 39/60\n",
      "30/30 - 8s - loss: 0.0030 - lwlrap: 0.9994 - val_loss: 0.0232 - val_lwlrap: 0.8840\n",
      "Epoch 40/60\n",
      "30/30 - 8s - loss: 0.0041 - lwlrap: 0.9995 - val_loss: 0.0217 - val_lwlrap: 0.8906\n",
      "Epoch 41/60\n",
      "30/30 - 8s - loss: 0.0083 - lwlrap: 0.9984 - val_loss: 0.0244 - val_lwlrap: 0.8818\n",
      "Epoch 42/60\n",
      "30/30 - 8s - loss: 0.0015 - lwlrap: 0.9980 - val_loss: 0.0350 - val_lwlrap: 0.8860\n",
      "Epoch 43/60\n",
      "30/30 - 8s - loss: 0.0048 - lwlrap: 0.9989 - val_loss: 0.0216 - val_lwlrap: 0.8926\n",
      "Epoch 44/60\n",
      "30/30 - 8s - loss: 0.0021 - lwlrap: 0.9973 - val_loss: 0.0188 - val_lwlrap: 0.8854\n",
      "\n",
      "-------------   Fold 5 / 5  -------------\n",
      "\n",
      " -> Preparing Data \n",
      "\n",
      " -> Preparing Model \n",
      "\n",
      " -> Loading weights from ../logs/2021-01-31/8/pretrained_best_fold4.h5\n",
      "\n",
      " -> Training Model \n",
      "\n",
      "Epoch 1/60\n",
      "30/30 - 10s - loss: 0.2246 - lwlrap: 0.3540 - val_loss: 0.1344 - val_lwlrap: 0.5019\n",
      "Epoch 2/60\n",
      "30/30 - 8s - loss: 0.2166 - lwlrap: 0.6809 - val_loss: 0.1133 - val_lwlrap: 0.6438\n",
      "Epoch 3/60\n",
      "30/30 - 8s - loss: 0.1002 - lwlrap: 0.8200 - val_loss: 0.1002 - val_lwlrap: 0.7729\n",
      "Epoch 4/60\n",
      "30/30 - 8s - loss: 0.1343 - lwlrap: 0.8982 - val_loss: 0.0724 - val_lwlrap: 0.7979\n",
      "Epoch 5/60\n",
      "30/30 - 8s - loss: 0.0775 - lwlrap: 0.9202 - val_loss: 0.0911 - val_lwlrap: 0.8212\n",
      "Epoch 6/60\n",
      "30/30 - 8s - loss: 0.1021 - lwlrap: 0.9002 - val_loss: 0.0857 - val_lwlrap: 0.7703\n",
      "Epoch 7/60\n",
      "30/30 - 8s - loss: 0.1683 - lwlrap: 0.8868 - val_loss: 0.0722 - val_lwlrap: 0.6764\n",
      "Epoch 8/60\n",
      "30/30 - 8s - loss: 0.1182 - lwlrap: 0.8995 - val_loss: 0.1235 - val_lwlrap: 0.7405\n",
      "Epoch 9/60\n",
      "30/30 - 8s - loss: 0.0592 - lwlrap: 0.9375 - val_loss: 0.0716 - val_lwlrap: 0.7932\n",
      "Epoch 10/60\n",
      "30/30 - 8s - loss: 0.0334 - lwlrap: 0.9332 - val_loss: 0.0941 - val_lwlrap: 0.7873\n",
      "Epoch 11/60\n",
      "30/30 - 8s - loss: 0.1038 - lwlrap: 0.9320 - val_loss: 0.0683 - val_lwlrap: 0.8336\n",
      "Epoch 12/60\n",
      "30/30 - 9s - loss: 0.0331 - lwlrap: 0.9633 - val_loss: 0.0793 - val_lwlrap: 0.8400\n",
      "Epoch 13/60\n",
      "30/30 - 8s - loss: 0.0302 - lwlrap: 0.9831 - val_loss: 0.0675 - val_lwlrap: 0.8538\n",
      "Epoch 14/60\n",
      "30/30 - 8s - loss: 0.0731 - lwlrap: 0.9709 - val_loss: 0.0747 - val_lwlrap: 0.8389\n",
      "Epoch 15/60\n",
      "30/30 - 8s - loss: 0.0095 - lwlrap: 0.9884 - val_loss: 0.0702 - val_lwlrap: 0.8503\n",
      "Epoch 16/60\n",
      "30/30 - 8s - loss: 0.0117 - lwlrap: 0.9929 - val_loss: 0.0626 - val_lwlrap: 0.8747\n",
      "Epoch 17/60\n",
      "30/30 - 8s - loss: 0.0132 - lwlrap: 0.9920 - val_loss: 0.0652 - val_lwlrap: 0.8569\n",
      "Epoch 18/60\n",
      "30/30 - 8s - loss: 0.0178 - lwlrap: 0.9861 - val_loss: 0.0887 - val_lwlrap: 0.8782\n",
      "Epoch 19/60\n",
      "30/30 - 8s - loss: 0.0064 - lwlrap: 0.9968 - val_loss: 0.0599 - val_lwlrap: 0.8740\n",
      "Epoch 20/60\n",
      "30/30 - 8s - loss: 0.0042 - lwlrap: 0.9961 - val_loss: 0.0511 - val_lwlrap: 0.8674\n",
      "Epoch 21/60\n",
      "30/30 - 8s - loss: 0.0023 - lwlrap: 0.9974 - val_loss: 0.0728 - val_lwlrap: 0.8712\n",
      "Epoch 22/60\n",
      "30/30 - 8s - loss: 0.0137 - lwlrap: 0.9967 - val_loss: 0.0706 - val_lwlrap: 0.8808\n",
      "Epoch 23/60\n",
      "30/30 - 8s - loss: 0.0051 - lwlrap: 0.9981 - val_loss: 0.0790 - val_lwlrap: 0.8810\n",
      "Epoch 24/60\n",
      "30/30 - 8s - loss: 0.0080 - lwlrap: 0.9952 - val_loss: 0.0642 - val_lwlrap: 0.8652\n",
      "Epoch 25/60\n",
      "30/30 - 8s - loss: 0.0025 - lwlrap: 0.9980 - val_loss: 0.0785 - val_lwlrap: 0.8619\n",
      "Epoch 26/60\n",
      "30/30 - 8s - loss: 0.0030 - lwlrap: 0.9987 - val_loss: 0.0860 - val_lwlrap: 0.8634\n",
      "Epoch 27/60\n",
      "30/30 - 8s - loss: 0.0063 - lwlrap: 0.9980 - val_loss: 0.0804 - val_lwlrap: 0.8614\n",
      "Epoch 28/60\n",
      "30/30 - 8s - loss: 0.0124 - lwlrap: 0.9938 - val_loss: 0.0805 - val_lwlrap: 0.8275\n",
      "Epoch 29/60\n",
      "30/30 - 8s - loss: 0.0202 - lwlrap: 0.9921 - val_loss: 0.0882 - val_lwlrap: 0.8417\n",
      "Epoch 30/60\n",
      "30/30 - 8s - loss: 0.0259 - lwlrap: 0.9913 - val_loss: 0.1020 - val_lwlrap: 0.8262\n",
      "Epoch 31/60\n",
      "30/30 - 8s - loss: 0.0102 - lwlrap: 0.9927 - val_loss: 0.0494 - val_lwlrap: 0.8600\n",
      "Epoch 32/60\n",
      "30/30 - 8s - loss: 0.0217 - lwlrap: 0.9910 - val_loss: 0.0552 - val_lwlrap: 0.8560\n",
      "Epoch 33/60\n",
      "30/30 - 8s - loss: 0.0249 - lwlrap: 0.9973 - val_loss: 0.0750 - val_lwlrap: 0.8639\n",
      "Epoch 34/60\n",
      "30/30 - 8s - loss: 0.0046 - lwlrap: 0.9945 - val_loss: 0.0525 - val_lwlrap: 0.8751\n",
      "Epoch 35/60\n",
      "30/30 - 8s - loss: 0.0011 - lwlrap: 0.9984 - val_loss: 0.0745 - val_lwlrap: 0.8628\n",
      "Epoch 36/60\n",
      "30/30 - 8s - loss: 0.0067 - lwlrap: 0.9979 - val_loss: 0.0677 - val_lwlrap: 0.8725\n",
      "Epoch 37/60\n",
      "30/30 - 8s - loss: 0.0205 - lwlrap: 0.9991 - val_loss: 0.0787 - val_lwlrap: 0.8655\n",
      "Epoch 38/60\n",
      "30/30 - 8s - loss: 0.0076 - lwlrap: 0.9986 - val_loss: 0.0807 - val_lwlrap: 0.8431\n",
      "Epoch 39/60\n",
      "30/30 - 8s - loss: 0.0016 - lwlrap: 0.9990 - val_loss: 0.0830 - val_lwlrap: 0.8510\n",
      "Epoch 40/60\n",
      "30/30 - 8s - loss: 0.0018 - lwlrap: 0.9975 - val_loss: 0.1015 - val_lwlrap: 0.8571\n",
      "Epoch 41/60\n",
      "30/30 - 8s - loss: 0.0018 - lwlrap: 0.9996 - val_loss: 0.0863 - val_lwlrap: 0.8542\n",
      "Epoch 42/60\n",
      "30/30 - 8s - loss: 0.0020 - lwlrap: 0.9995 - val_loss: 0.0738 - val_lwlrap: 0.8702\n",
      "Epoch 43/60\n",
      "30/30 - 8s - loss: 0.0067 - lwlrap: 0.9985 - val_loss: 0.0870 - val_lwlrap: 0.8691\n"
     ]
    }
   ],
   "source": [
    "if not DEBUG:\n",
    "    log_folder = prepare_log_folder(LOG_PATH)\n",
    "    print(f'Logging results to {log_folder}')\n",
    "    create_logger(directory=log_folder, name=\"logs.txt\")\n",
    "    \n",
    "for fold_idx in range(5):\n",
    "    print(f\"\\n-------------   Fold {fold_idx + 1} / {5}  -------------\\n\")\n",
    "    \n",
    "    main(\n",
    "        fold_idx, \n",
    "        log_folder, \n",
    "        pretrained_folder + f\"pretrained_best_fold{fold_idx}.h5\", \n",
    "        pretrained_with_contrastive=False\n",
    "    )\n",
    "    \n",
    "    if DEBUG:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
